{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:06:33.124625Z",
     "start_time": "2020-12-09T20:06:32.407521Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "from gpytorch.likelihoods import _GaussianLikelihoodBase\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from gpytorch.distributions import MultivariateNormal, base_distributions\n",
    "from gpytorch.lazy import ZeroLazyTensor\n",
    "from gpytorch.utils.warnings import GPInputWarning\n",
    "from gpytorch.likelihoods.likelihood import Likelihood\n",
    "from gpytorch.likelihoods.noise_models import FixedGaussianNoise, HomoskedasticNoise, Noise\n",
    "from typing import Any, Optional\n",
    "from gpytorch.mlls._approximate_mll import _ApproximateMarginalLogLikelihood\n",
    "\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from gpytorch.distributions import base_distributions\n",
    "from gpytorch.functions import add_diag\n",
    "from gpytorch.lazy import (\n",
    "    BlockDiagLazyTensor,\n",
    "    DiagLazyTensor,\n",
    "    KroneckerProductLazyTensor,\n",
    "    MatmulLazyTensor,\n",
    "    RootLazyTensor,\n",
    "    lazify,\n",
    ")\n",
    "from gpytorch.likelihoods import Likelihood, _GaussianLikelihoodBase\n",
    "from gpytorch.utils.warnings import OldVersionWarning\n",
    "from gpytorch.likelihoods.noise_models import MultitaskHomoskedasticNoise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:06:33.129262Z",
     "start_time": "2020-12-09T20:06:33.125710Z"
    }
   },
   "outputs": [],
   "source": [
    "train_T = 65000\n",
    "test_T = 15000\n",
    "N = 200\n",
    "M = 25\n",
    "batch_size = 1500\n",
    "\n",
    "C_den = torch.zeros(5,5)\n",
    "C_den[0,1:] = 1\n",
    "\n",
    "sub_no = C_den.shape[0]\n",
    "num_tasks = sub_no * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:06:33.168399Z",
     "start_time": "2020-12-09T20:06:33.131208Z"
    },
    "code_folding": [
     0,
     187
    ]
   },
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, num_tasks, M):\n",
    "        # Let's use a different set of inducing points for each task\n",
    "        inducing_points = torch.rand(num_tasks, M, 1)\n",
    "\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([num_tasks])\n",
    "        )\n",
    "\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks=num_tasks,\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_tasks]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_tasks])),\n",
    "            batch_shape=torch.Size([num_tasks])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class CustomLikelihood( _GaussianLikelihoodBase):\n",
    "    def __init__(self, C_den, sub_no, N, num_tasks,\n",
    "        rank=0,\n",
    "        task_correlation_prior=None,\n",
    "        batch_shape=torch.Size(),\n",
    "        noise_prior=None,\n",
    "        noise_constraint=None):\n",
    "        if noise_constraint is None:\n",
    "            noise_constraint = GreaterThan(1e-4)\n",
    "\n",
    "        noise_covar = MultitaskHomoskedasticNoise(\n",
    "            num_tasks=num_tasks, noise_prior=noise_prior, noise_constraint=noise_constraint, batch_shape=batch_shape\n",
    "        )\n",
    "        super().__init__(noise_covar=noise_covar)\n",
    "        if rank != 0:\n",
    "            if rank > num_tasks:\n",
    "                raise ValueError(f\"Cannot have rank ({rank}) greater than num_tasks ({num_tasks})\")\n",
    "            tidcs = torch.tril_indices(num_tasks, rank, dtype=torch.long)\n",
    "            self.tidcs = tidcs[:, 1:]  # (1, 1) must be 1.0, no need to parameterize this\n",
    "            task_noise_corr = torch.randn(*batch_shape, self.tidcs.size(-1))\n",
    "            self.register_parameter(\"task_noise_corr\", torch.nn.Parameter(task_noise_corr))\n",
    "            if task_correlation_prior is not None:\n",
    "                self.register_prior(\n",
    "                    \"MultitaskErrorCorrelationPrior\", task_correlation_prior, lambda: self._eval_corr_matrix\n",
    "                )\n",
    "        elif task_correlation_prior is not None:\n",
    "            raise ValueError(\"Can only specify task_correlation_prior if rank>0\")\n",
    "        self.num_tasks = num_tasks\n",
    "        self.rank = rank\n",
    "                \n",
    "        self.C_den = C_den\n",
    "        self.sub_no = sub_no\n",
    "        self.N = N\n",
    "        \n",
    "        # Between Subunit Parameters\n",
    "        self.W_log = nn.Parameter(torch.randn(self.sub_no) , requires_grad=True) # POSITIVE\n",
    "\n",
    "        ### Subunit Output Parameters ###\n",
    "        self.V_o = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "        self.Theta = nn.Parameter(torch.zeros(self.sub_no), requires_grad=True)\n",
    "    \n",
    "    @property\n",
    "    def noise(self):\n",
    "        return self.raw_noise_constraint.transform(self.raw_noise)\n",
    "\n",
    "    @noise.setter\n",
    "    def noise(self, value):\n",
    "        self._set_noise(value)\n",
    "\n",
    "    def _set_noise(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_noise)\n",
    "        self.initialize(raw_noise=self.raw_noise_constraint.inverse_transform(value))\n",
    "\n",
    "    def _shaped_noise_covar(self, base_shape, *params):\n",
    "        if len(base_shape) >= 2:\n",
    "            *batch_shape, n, _ = base_shape\n",
    "        else:\n",
    "            *batch_shape, n = base_shape\n",
    "\n",
    "        # compute the noise covariance\n",
    "        if len(params) > 0:\n",
    "            shape = None\n",
    "        else:\n",
    "            shape = base_shape if len(base_shape) == 1 else base_shape[:-1]\n",
    "        noise_covar = self.noise_covar(*params, shape=shape)\n",
    "\n",
    "        if self.rank > 0:\n",
    "            # if rank > 0, compute the task correlation matrix\n",
    "            # TODO: This is inefficient, change repeat so it can repeat LazyTensors w/ multiple batch dimensions\n",
    "            task_corr = self._eval_corr_matrix()\n",
    "            exp_shape = torch.Size([*batch_shape, n]) + task_corr.shape[-2:]\n",
    "            task_corr_exp = lazify(task_corr.unsqueeze(-3).expand(exp_shape))\n",
    "            noise_sem = noise_covar.sqrt()\n",
    "            task_covar_blocks = MatmulLazyTensor(MatmulLazyTensor(noise_sem, task_corr_exp), noise_sem)\n",
    "        else:\n",
    "            # otherwise tasks are uncorrelated\n",
    "            task_covar_blocks = noise_covar\n",
    "\n",
    "        if len(batch_shape) == 1:\n",
    "            # TODO: Properly support general batch shapes in BlockDiagLazyTensor (no shape arithmetic)\n",
    "            tcb_eval = task_covar_blocks.evaluate()\n",
    "            task_covar = BlockDiagLazyTensor(lazify(tcb_eval), block_dim=-3)\n",
    "        else:\n",
    "            task_covar = BlockDiagLazyTensor(task_covar_blocks)\n",
    "\n",
    "        return task_covar\n",
    "        \n",
    "    def expected_log_prob(self, target: Tensor, input: MultivariateNormal, S_e, S_i, *params: Any, **kwargs: Any) -> Tensor:\n",
    "        #mean, variance = input.mean, input.variance\n",
    "        #noise = self._shaped_noise_covar(mean.shape, *params, **kwargs).diag()\n",
    "        # Potentially reshape the noise to deal with the multitask case\n",
    "        #noise = noise.view(*noise.shape[:-1], *input.event_shape)\n",
    "        \n",
    "     \n",
    "        \n",
    "        all_F = input.mean.T + torch.sqrt(input.variance.T)\n",
    "        all_F = all_F * 0.01\n",
    "        T = S_e.shape[0]\n",
    "        \n",
    "        F_e = all_F[:self.sub_no].unsqueeze(1)\n",
    "        F_i = all_F[self.sub_no:].unsqueeze(1)\n",
    "        #flip_F_e = torch.flip(F_e, [2])\n",
    "        #flip_F_i = torch.flip(F_i, [2])\n",
    "        flip_F_e = F_e\n",
    "        flip_F_i = F_i\n",
    "        \n",
    "        pad_S_e = torch.zeros(T + self.N-1, self.sub_no).cuda()\n",
    "        pad_S_i = torch.zeros(T + self.N-1, self.sub_no).cuda()\n",
    "        pad_S_e[-T:] = pad_S_e[-T:] + S_e\n",
    "        pad_S_i[-T:] = pad_S_i[-T:] + S_i\n",
    "        pad_S_e = pad_S_e.T.unsqueeze(0)\n",
    "        pad_S_i = pad_S_i.T.unsqueeze(0)\n",
    "\n",
    "        filtered_e = F.conv1d(pad_S_e, flip_F_e, padding=0, groups=self.sub_no).squeeze(0).T\n",
    "        filtered_i = F.conv1d(pad_S_i, flip_F_i, padding=0, groups=self.sub_no).squeeze(0).T\n",
    "\n",
    "        syn_in = filtered_e + filtered_i\n",
    "\n",
    "        #----- Combine Subunits -----#\n",
    "\n",
    "        sub_out = torch.zeros(T, self.sub_no).cuda()\n",
    "        \n",
    "        for s in range(self.sub_no):\n",
    "            sub_idx = -s-1\n",
    "            leaf_idx = torch.where(self.C_den[sub_idx] == 1)[0]\n",
    "\n",
    "            if torch.numel(leaf_idx) == 0:\n",
    "                nonlin_out = torch.tanh(syn_in[:,sub_idx] + self.Theta[sub_idx]) # (T_data,) \n",
    "                sub_out[:,sub_idx] = sub_out[:,sub_idx] + nonlin_out\n",
    "            else:\n",
    "                leaf_in = sub_out[:,leaf_idx] * torch.exp(self.W_log[leaf_idx]) # (T_data,)\n",
    "                nonlin_in = syn_in[:,sub_idx] + torch.sum(leaf_in, 1) + self.Theta[sub_idx]# (T_data,)\n",
    "                nonlin_out = torch.tanh(nonlin_in)\n",
    "                sub_out[:,sub_idx] = sub_out[:,sub_idx] + nonlin_out\n",
    "        \n",
    "        final_voltage = sub_out[:,0]*torch.exp(self.W_log[0]) + self.V_o\n",
    "\n",
    "        #res = (target - final_voltage) ** 2\n",
    "        #res = res.mul(-0.5)\n",
    "        res = torch.var(target - final_voltage)\n",
    "        \n",
    "        return res, final_voltage\n",
    "    \n",
    "class VariationalELBO(_ApproximateMarginalLogLikelihood):\n",
    "    def _log_likelihood_term(self, variational_dist_f, target, S_e, S_i, **kwargs):\n",
    "        error, pred = self.likelihood.expected_log_prob(target, variational_dist_f, S_e, S_i, **kwargs)\n",
    "        \n",
    "        return error.sum(-1), pred\n",
    "\n",
    "    def forward(self, approximate_dist_f, target, S_e, S_i, **kwargs):\n",
    "        r\"\"\"\n",
    "        Computes the Variational ELBO given :math:`q(\\mathbf f)` and `\\mathbf y`.\n",
    "        Calling this function will call the likelihood's `expected_log_prob` function.\n",
    "        Args:\n",
    "            :attr:`approximate_dist_f` (:obj:`gpytorch.distributions.MultivariateNormal`):\n",
    "                :math:`q(\\mathbf f)` the outputs of the latent function (the :obj:`gpytorch.models.ApproximateGP`)\n",
    "            :attr:`target` (`torch.Tensor`):\n",
    "                :math:`\\mathbf y` The target values\n",
    "            :attr:`**kwargs`:\n",
    "                Additional arguments passed to the likelihood's `expected_log_prob` function.\n",
    "        \"\"\"\n",
    "        # Get likelihood term and KL term\n",
    "        num_batch = approximate_dist_f.event_shape[0]\n",
    "        log_likelihood, pred = self._log_likelihood_term(approximate_dist_f, target, S_e, S_i,**kwargs)\n",
    "        log_likelihood = log_likelihood.div(num_batch)\n",
    "        \n",
    "        kl_divergence = self.model.variational_strategy.kl_divergence().div(self.num_data / self.beta)\n",
    "\n",
    "        # Add any additional registered loss terms\n",
    "        added_loss = torch.zeros_like(log_likelihood)\n",
    "        had_added_losses = False\n",
    "        for added_loss_term in self.model.added_loss_terms():\n",
    "            added_loss.add_(added_loss_term.loss())\n",
    "            had_added_losses = True\n",
    "\n",
    "        # Log prior term\n",
    "        log_prior = torch.zeros_like(log_likelihood)\n",
    "        for _, prior, closure, _ in self.named_priors():\n",
    "            log_prior.add_(prior.log_prob(closure()).sum().div(self.num_data))\n",
    "\n",
    "        if self.combine_terms:\n",
    "            return log_likelihood - kl_divergence + log_prior - added_loss , pred\n",
    "            #return log_likelihood , pred\n",
    "        else:\n",
    "            if had_added_losses:\n",
    "                return log_likelihood, kl_divergence, log_prior.div(self.num_data), added_loss\n",
    "            else:\n",
    "                return log_likelihood, kl_divergence, log_prior.div(self.num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:06:33.181205Z",
     "start_time": "2020-12-09T20:06:33.169653Z"
    }
   },
   "outputs": [],
   "source": [
    "Ensyn = torch.tensor([0, 106, 213, 211, 99])\n",
    "Insyn = torch.tensor([1, 22, 36, 42, 19])\n",
    "\n",
    "E_no = torch.sum(Ensyn)\n",
    "I_no = torch.sum(Insyn)\n",
    "\n",
    "C_syn_e = torch.zeros(sub_no, E_no)\n",
    "C_syn_i = torch.zeros(sub_no, I_no)\n",
    "\n",
    "E_count = 0\n",
    "for s in range(sub_no):\n",
    "    C_syn_e[s,E_count:E_count+Ensyn[s]] = 1\n",
    "    E_count += Ensyn[s]\n",
    "\n",
    "I_count = 0\n",
    "for s in range(sub_no):\n",
    "    C_syn_i[s,I_count:I_count+Insyn[s]] = 1\n",
    "    I_count += Insyn[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:06:35.495809Z",
     "start_time": "2020-12-09T20:06:33.182679Z"
    }
   },
   "outputs": [],
   "source": [
    "V_ref = np.load(\"/media/hdd01/sklee/L23_inputs/vdata_NMDA_ApN0.5_13_Adend_r0_o2_i2_g_b4.npy\").flatten()\n",
    "\n",
    "train_V_ref = V_ref[:train_T]\n",
    "test_V_ref = V_ref[train_T:train_T+test_T]\n",
    "test_V_ref = torch.from_numpy(test_V_ref).cuda()\n",
    "train_V_ref = torch.from_numpy(train_V_ref).cuda()\n",
    "\n",
    "raw_E_neural = np.load(\"/media/hdd01/sklee/L23_inputs/Espikes_NMDA_ApN0.5_13_Adend_r0_o2_i2_g_b4_neural.npy\")\n",
    "raw_I_neural = np.load(\"/media/hdd01/sklee/L23_inputs/Ispikes_NMDA_ApN0.5_13_Adend_r0_o2_i2_g_b4_neural.npy\")\n",
    "\n",
    "E_neural = torch.matmul(torch.from_numpy(raw_E_neural).double(), C_syn_e.T.double())\n",
    "I_neural = torch.matmul(torch.from_numpy(raw_I_neural).double(), C_syn_i.T.double())\n",
    "\n",
    "train_S_E = E_neural[:train_T].cuda()\n",
    "train_S_I = I_neural[:train_T].cuda()\n",
    "test_S_E = E_neural[train_T:train_T+test_T].double().cuda()\n",
    "test_S_I = I_neural[train_T:train_T+test_T].double().cuda()\n",
    "\n",
    "repeat_no = 1\n",
    "batch_no = (train_V_ref.shape[0] - batch_size) * repeat_no\n",
    "train_idx = np.empty((repeat_no, train_V_ref.shape[0] - batch_size))\n",
    "for i in range(repeat_no):\n",
    "    part_idx = np.arange(train_V_ref.shape[0] - batch_size)\n",
    "    np.random.shuffle(part_idx)\n",
    "    train_idx[i] = part_idx\n",
    "train_idx = train_idx.flatten()\n",
    "train_idx = torch.from_numpy(train_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:06:35.502398Z",
     "start_time": "2020-12-09T20:06:35.497035Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "model = MultitaskGPModel(num_tasks, M)\n",
    "likelihood = CustomLikelihood(C_den.cuda(), sub_no, N, num_tasks)\n",
    "\n",
    "num_epochs = 10000\n",
    "model.cuda().train()\n",
    "likelihood.cuda().train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr = 0.005)\n",
    "\n",
    "#lr = 0.00004\n",
    "\n",
    "train_x = torch.arange(N).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T20:06:32.282Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.118211741070525e-05 1607544391.9688473\n"
     ]
    }
   ],
   "source": [
    "mll = VariationalELBO(likelihood, model, num_data=train_V_ref.shape[0])\n",
    "#mll = VariationalELBO(likelihood, model, num_data=N)\n",
    "#epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss, pred = mll(output, train_V_ref, train_S_E, train_S_I)\n",
    "    #epochs_iter.set_postfix(loss=loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if count%200 == 0:\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        test_output = model(train_x)\n",
    "        test_loss, test_pred = mll(test_output, test_V_ref, test_S_E, test_S_I)\n",
    "        \n",
    "        \n",
    "        test_score = metrics.explained_variance_score(y_true=test_V_ref.cpu().detach().numpy(),\n",
    "                                                      y_pred=test_pred.cpu().detach().numpy(),\n",
    "                                                      multioutput='uniform_average')\n",
    "        print(count, test_score, time.time() - s)\n",
    "        s = time.time()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T18:07:53.852897Z",
     "start_time": "2020-12-09T18:07:53.842997Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T20:06:32.283Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(test_V_ref.cpu().detach().numpy()[1000:4000])\n",
    "plt.plot(test_pred.cpu().detach().numpy()[1000:4000]-61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T20:06:32.284Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "F = test_output.mean.T + torch.sqrt(test_output.variance.T)\n",
    "plt.plot(F[6].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
