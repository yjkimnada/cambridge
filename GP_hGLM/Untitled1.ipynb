{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:08:24.524417Z",
     "start_time": "2020-12-09T20:08:23.798969Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "from gpytorch.likelihoods import _GaussianLikelihoodBase\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from gpytorch.distributions import MultivariateNormal, base_distributions\n",
    "from gpytorch.lazy import ZeroLazyTensor\n",
    "from gpytorch.utils.warnings import GPInputWarning\n",
    "from gpytorch.likelihoods.likelihood import Likelihood\n",
    "from gpytorch.likelihoods.noise_models import FixedGaussianNoise, HomoskedasticNoise, Noise\n",
    "from typing import Any, Optional\n",
    "from gpytorch.mlls._approximate_mll import _ApproximateMarginalLogLikelihood\n",
    "\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from gpytorch.distributions import base_distributions\n",
    "from gpytorch.functions import add_diag\n",
    "from gpytorch.lazy import (\n",
    "    BlockDiagLazyTensor,\n",
    "    DiagLazyTensor,\n",
    "    KroneckerProductLazyTensor,\n",
    "    MatmulLazyTensor,\n",
    "    RootLazyTensor,\n",
    "    lazify,\n",
    ")\n",
    "from gpytorch.likelihoods import Likelihood, _GaussianLikelihoodBase\n",
    "from gpytorch.utils.warnings import OldVersionWarning\n",
    "from gpytorch.likelihoods.noise_models import MultitaskHomoskedasticNoise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:08:24.528553Z",
     "start_time": "2020-12-09T20:08:24.525527Z"
    }
   },
   "outputs": [],
   "source": [
    "train_T = 65000\n",
    "test_T = 15000\n",
    "N = 200\n",
    "M = 25\n",
    "batch_size = 1500\n",
    "\n",
    "C_den = torch.zeros(5,5)\n",
    "C_den[0,1:] = 1\n",
    "\n",
    "sub_no = C_den.shape[0]\n",
    "num_tasks = sub_no * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:08:24.564920Z",
     "start_time": "2020-12-09T20:08:24.530254Z"
    },
    "code_folding": [
     0,
     187
    ]
   },
   "outputs": [],
   "source": [
    "class MultitaskGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, num_tasks, M):\n",
    "        # Let's use a different set of inducing points for each task\n",
    "        inducing_points = torch.rand(num_tasks, M, 1)\n",
    "\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([num_tasks])\n",
    "        )\n",
    "\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks=num_tasks,\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_tasks]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_tasks])),\n",
    "            batch_shape=torch.Size([num_tasks])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class CustomLikelihood( _GaussianLikelihoodBase):\n",
    "    def __init__(self, C_den, sub_no, N, num_tasks,\n",
    "        rank=0,\n",
    "        task_correlation_prior=None,\n",
    "        batch_shape=torch.Size(),\n",
    "        noise_prior=None,\n",
    "        noise_constraint=None):\n",
    "        if noise_constraint is None:\n",
    "            noise_constraint = GreaterThan(1e-4)\n",
    "\n",
    "        noise_covar = MultitaskHomoskedasticNoise(\n",
    "            num_tasks=num_tasks, noise_prior=noise_prior, noise_constraint=noise_constraint, batch_shape=batch_shape\n",
    "        )\n",
    "        super().__init__(noise_covar=noise_covar)\n",
    "        if rank != 0:\n",
    "            if rank > num_tasks:\n",
    "                raise ValueError(f\"Cannot have rank ({rank}) greater than num_tasks ({num_tasks})\")\n",
    "            tidcs = torch.tril_indices(num_tasks, rank, dtype=torch.long)\n",
    "            self.tidcs = tidcs[:, 1:]  # (1, 1) must be 1.0, no need to parameterize this\n",
    "            task_noise_corr = torch.randn(*batch_shape, self.tidcs.size(-1))\n",
    "            self.register_parameter(\"task_noise_corr\", torch.nn.Parameter(task_noise_corr))\n",
    "            if task_correlation_prior is not None:\n",
    "                self.register_prior(\n",
    "                    \"MultitaskErrorCorrelationPrior\", task_correlation_prior, lambda: self._eval_corr_matrix\n",
    "                )\n",
    "        elif task_correlation_prior is not None:\n",
    "            raise ValueError(\"Can only specify task_correlation_prior if rank>0\")\n",
    "        self.num_tasks = num_tasks\n",
    "        self.rank = rank\n",
    "                \n",
    "        self.C_den = C_den\n",
    "        self.sub_no = sub_no\n",
    "        self.N = N\n",
    "        \n",
    "        # Between Subunit Parameters\n",
    "        self.W_log = nn.Parameter(torch.randn(self.sub_no) , requires_grad=True) # POSITIVE\n",
    "\n",
    "        ### Subunit Output Parameters ###\n",
    "        self.V_o = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "        self.Theta = nn.Parameter(torch.zeros(self.sub_no), requires_grad=True)\n",
    "    \n",
    "    @property\n",
    "    def noise(self):\n",
    "        return self.raw_noise_constraint.transform(self.raw_noise)\n",
    "\n",
    "    @noise.setter\n",
    "    def noise(self, value):\n",
    "        self._set_noise(value)\n",
    "\n",
    "    def _set_noise(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_noise)\n",
    "        self.initialize(raw_noise=self.raw_noise_constraint.inverse_transform(value))\n",
    "\n",
    "    def _shaped_noise_covar(self, base_shape, *params):\n",
    "        if len(base_shape) >= 2:\n",
    "            *batch_shape, n, _ = base_shape\n",
    "        else:\n",
    "            *batch_shape, n = base_shape\n",
    "\n",
    "        # compute the noise covariance\n",
    "        if len(params) > 0:\n",
    "            shape = None\n",
    "        else:\n",
    "            shape = base_shape if len(base_shape) == 1 else base_shape[:-1]\n",
    "        noise_covar = self.noise_covar(*params, shape=shape)\n",
    "\n",
    "        if self.rank > 0:\n",
    "            # if rank > 0, compute the task correlation matrix\n",
    "            # TODO: This is inefficient, change repeat so it can repeat LazyTensors w/ multiple batch dimensions\n",
    "            task_corr = self._eval_corr_matrix()\n",
    "            exp_shape = torch.Size([*batch_shape, n]) + task_corr.shape[-2:]\n",
    "            task_corr_exp = lazify(task_corr.unsqueeze(-3).expand(exp_shape))\n",
    "            noise_sem = noise_covar.sqrt()\n",
    "            task_covar_blocks = MatmulLazyTensor(MatmulLazyTensor(noise_sem, task_corr_exp), noise_sem)\n",
    "        else:\n",
    "            # otherwise tasks are uncorrelated\n",
    "            task_covar_blocks = noise_covar\n",
    "\n",
    "        if len(batch_shape) == 1:\n",
    "            # TODO: Properly support general batch shapes in BlockDiagLazyTensor (no shape arithmetic)\n",
    "            tcb_eval = task_covar_blocks.evaluate()\n",
    "            task_covar = BlockDiagLazyTensor(lazify(tcb_eval), block_dim=-3)\n",
    "        else:\n",
    "            task_covar = BlockDiagLazyTensor(task_covar_blocks)\n",
    "\n",
    "        return task_covar\n",
    "        \n",
    "    def expected_log_prob(self, target: Tensor, input: MultivariateNormal, S_e, S_i, *params: Any, **kwargs: Any) -> Tensor:\n",
    "        #mean, variance = input.mean, input.variance\n",
    "        #noise = self._shaped_noise_covar(mean.shape, *params, **kwargs).diag()\n",
    "        # Potentially reshape the noise to deal with the multitask case\n",
    "        #noise = noise.view(*noise.shape[:-1], *input.event_shape)\n",
    "        \n",
    "     \n",
    "        \n",
    "        all_F = input.mean.T + torch.sqrt(input.variance.T)\n",
    "        all_F = all_F * 0.01\n",
    "        T = S_e.shape[0]\n",
    "        \n",
    "        F_e = all_F[:self.sub_no].unsqueeze(1)\n",
    "        F_i = all_F[self.sub_no:].unsqueeze(1)\n",
    "        #flip_F_e = torch.flip(F_e, [2])\n",
    "        #flip_F_i = torch.flip(F_i, [2])\n",
    "        flip_F_e = F_e\n",
    "        flip_F_i = F_i\n",
    "        \n",
    "        pad_S_e = torch.zeros(T + self.N-1, self.sub_no).cuda()\n",
    "        pad_S_i = torch.zeros(T + self.N-1, self.sub_no).cuda()\n",
    "        pad_S_e[-T:] = pad_S_e[-T:] + S_e\n",
    "        pad_S_i[-T:] = pad_S_i[-T:] + S_i\n",
    "        pad_S_e = pad_S_e.T.unsqueeze(0)\n",
    "        pad_S_i = pad_S_i.T.unsqueeze(0)\n",
    "\n",
    "        filtered_e = F.conv1d(pad_S_e, flip_F_e, padding=0, groups=self.sub_no).squeeze(0).T\n",
    "        filtered_i = F.conv1d(pad_S_i, flip_F_i, padding=0, groups=self.sub_no).squeeze(0).T\n",
    "\n",
    "        syn_in = filtered_e + filtered_i\n",
    "\n",
    "        #----- Combine Subunits -----#\n",
    "\n",
    "        sub_out = torch.zeros(T, self.sub_no).cuda()\n",
    "        \n",
    "        for s in range(self.sub_no):\n",
    "            sub_idx = -s-1\n",
    "            leaf_idx = torch.where(self.C_den[sub_idx] == 1)[0]\n",
    "\n",
    "            if torch.numel(leaf_idx) == 0:\n",
    "                nonlin_out = torch.tanh(syn_in[:,sub_idx] + self.Theta[sub_idx]) # (T_data,) \n",
    "                sub_out[:,sub_idx] = sub_out[:,sub_idx] + nonlin_out\n",
    "            else:\n",
    "                leaf_in = sub_out[:,leaf_idx] * torch.exp(self.W_log[leaf_idx]) # (T_data,)\n",
    "                nonlin_in = syn_in[:,sub_idx] + torch.sum(leaf_in, 1) + self.Theta[sub_idx]# (T_data,)\n",
    "                nonlin_out = torch.tanh(nonlin_in)\n",
    "                sub_out[:,sub_idx] = sub_out[:,sub_idx] + nonlin_out\n",
    "        \n",
    "        final_voltage = sub_out[:,0]*torch.exp(self.W_log[0]) + self.V_o\n",
    "\n",
    "        #res = (target - final_voltage) ** 2\n",
    "        #res = res.mul(-0.5)\n",
    "        res = torch.var(target - final_voltage)\n",
    "        \n",
    "        return res, final_voltage\n",
    "    \n",
    "class VariationalELBO(_ApproximateMarginalLogLikelihood):\n",
    "    def _log_likelihood_term(self, variational_dist_f, target, S_e, S_i, **kwargs):\n",
    "        error, pred = self.likelihood.expected_log_prob(target, variational_dist_f, S_e, S_i, **kwargs)\n",
    "        \n",
    "        return error.sum(-1), pred\n",
    "\n",
    "    def forward(self, approximate_dist_f, target, S_e, S_i, **kwargs):\n",
    "        r\"\"\"\n",
    "        Computes the Variational ELBO given :math:`q(\\mathbf f)` and `\\mathbf y`.\n",
    "        Calling this function will call the likelihood's `expected_log_prob` function.\n",
    "        Args:\n",
    "            :attr:`approximate_dist_f` (:obj:`gpytorch.distributions.MultivariateNormal`):\n",
    "                :math:`q(\\mathbf f)` the outputs of the latent function (the :obj:`gpytorch.models.ApproximateGP`)\n",
    "            :attr:`target` (`torch.Tensor`):\n",
    "                :math:`\\mathbf y` The target values\n",
    "            :attr:`**kwargs`:\n",
    "                Additional arguments passed to the likelihood's `expected_log_prob` function.\n",
    "        \"\"\"\n",
    "        # Get likelihood term and KL term\n",
    "        num_batch = approximate_dist_f.event_shape[0]\n",
    "        log_likelihood, pred = self._log_likelihood_term(approximate_dist_f, target, S_e, S_i,**kwargs)\n",
    "        log_likelihood = log_likelihood.div(num_batch)\n",
    "        \n",
    "        kl_divergence = self.model.variational_strategy.kl_divergence().div(self.num_data / self.beta)\n",
    "\n",
    "        # Add any additional registered loss terms\n",
    "        added_loss = torch.zeros_like(log_likelihood)\n",
    "        had_added_losses = False\n",
    "        for added_loss_term in self.model.added_loss_terms():\n",
    "            added_loss.add_(added_loss_term.loss())\n",
    "            had_added_losses = True\n",
    "\n",
    "        # Log prior term\n",
    "        log_prior = torch.zeros_like(log_likelihood)\n",
    "        for _, prior, closure, _ in self.named_priors():\n",
    "            log_prior.add_(prior.log_prob(closure()).sum().div(self.num_data))\n",
    "\n",
    "        if self.combine_terms:\n",
    "            return log_likelihood - kl_divergence + log_prior - added_loss , pred\n",
    "            #return log_likelihood , pred\n",
    "        else:\n",
    "            if had_added_losses:\n",
    "                return log_likelihood, kl_divergence, log_prior.div(self.num_data), added_loss\n",
    "            else:\n",
    "                return log_likelihood, kl_divergence, log_prior.div(self.num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:08:24.574338Z",
     "start_time": "2020-12-09T20:08:24.566087Z"
    }
   },
   "outputs": [],
   "source": [
    "Ensyn = torch.tensor([0, 106, 213, 211, 99])\n",
    "Insyn = torch.tensor([1, 22, 36, 42, 19])\n",
    "\n",
    "E_no = torch.sum(Ensyn)\n",
    "I_no = torch.sum(Insyn)\n",
    "\n",
    "C_syn_e = torch.zeros(sub_no, E_no)\n",
    "C_syn_i = torch.zeros(sub_no, I_no)\n",
    "\n",
    "E_count = 0\n",
    "for s in range(sub_no):\n",
    "    C_syn_e[s,E_count:E_count+Ensyn[s]] = 1\n",
    "    E_count += Ensyn[s]\n",
    "\n",
    "I_count = 0\n",
    "for s in range(sub_no):\n",
    "    C_syn_i[s,I_count:I_count+Insyn[s]] = 1\n",
    "    I_count += Insyn[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:08:26.981432Z",
     "start_time": "2020-12-09T20:08:24.575255Z"
    }
   },
   "outputs": [],
   "source": [
    "V_ref = np.load(\"/media/hdd01/sklee/L23_inputs/vdata_NMDA_ApN0.5_13_Adend_r0_o2_i2_g_b4.npy\").flatten()\n",
    "\n",
    "train_V_ref = V_ref[:train_T]\n",
    "test_V_ref = V_ref[train_T:train_T+test_T]\n",
    "test_V_ref = torch.from_numpy(test_V_ref).cuda()\n",
    "train_V_ref = torch.from_numpy(train_V_ref).cuda()\n",
    "\n",
    "raw_E_neural = np.load(\"/media/hdd01/sklee/L23_inputs/Espikes_NMDA_ApN0.5_13_Adend_r0_o2_i2_g_b4_neural.npy\")\n",
    "raw_I_neural = np.load(\"/media/hdd01/sklee/L23_inputs/Ispikes_NMDA_ApN0.5_13_Adend_r0_o2_i2_g_b4_neural.npy\")\n",
    "\n",
    "E_neural = torch.matmul(torch.from_numpy(raw_E_neural).double(), C_syn_e.T.double())\n",
    "I_neural = torch.matmul(torch.from_numpy(raw_I_neural).double(), C_syn_i.T.double())\n",
    "\n",
    "train_S_E = E_neural[:train_T].cuda()\n",
    "train_S_I = I_neural[:train_T].cuda()\n",
    "test_S_E = E_neural[train_T:train_T+test_T].double().cuda()\n",
    "test_S_I = I_neural[train_T:train_T+test_T].double().cuda()\n",
    "\n",
    "repeat_no = 1\n",
    "batch_no = (train_V_ref.shape[0] - batch_size) * repeat_no\n",
    "train_idx = np.empty((repeat_no, train_V_ref.shape[0] - batch_size))\n",
    "for i in range(repeat_no):\n",
    "    part_idx = np.arange(train_V_ref.shape[0] - batch_size)\n",
    "    np.random.shuffle(part_idx)\n",
    "    train_idx[i] = part_idx\n",
    "train_idx = train_idx.flatten()\n",
    "train_idx = torch.from_numpy(train_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T20:08:26.988525Z",
     "start_time": "2020-12-09T20:08:26.982482Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "model = MultitaskGPModel(num_tasks, M)\n",
    "likelihood = CustomLikelihood(C_den.cuda(), sub_no, N, num_tasks)\n",
    "\n",
    "num_epochs = 10000\n",
    "model.cuda().train()\n",
    "likelihood.cuda().train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr = 0.005)\n",
    "\n",
    "#lr = 0.00004\n",
    "\n",
    "train_x = torch.arange(N).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T20:08:23.671Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0002130494870529187 1607544503.4937158\n",
      "200 0.42898678838622495 4.276429891586304\n",
      "400 0.4623432614251073 3.96628475189209\n",
      "600 0.48096005207983494 4.385301828384399\n",
      "800 0.493454513447113 4.120760679244995\n",
      "1000 0.5014247455625364 4.298759460449219\n",
      "1200 0.5035808817924663 3.933112382888794\n",
      "1400 0.5106281111708169 3.931770086288452\n",
      "1600 0.5146903580874764 4.223888158798218\n",
      "1800 0.5154240581476999 4.4361841678619385\n",
      "2000 0.5176646431983605 4.4247448444366455\n",
      "2200 0.5165663930237814 4.358723402023315\n",
      "2400 0.5209506193609682 4.362861156463623\n",
      "2600 0.521330548335085 4.354378938674927\n",
      "2800 0.5224466795085673 4.2978761196136475\n",
      "3000 0.5198069044674694 4.281505346298218\n",
      "3200 0.5228890696304218 4.289567232131958\n",
      "3400 0.5233185811550405 4.23442530632019\n",
      "3600 0.5241169920658326 4.276432514190674\n",
      "3800 0.5242426272735772 4.3310911655426025\n",
      "4000 0.5226913464041494 4.312373638153076\n",
      "4200 0.528112912820859 4.295971155166626\n",
      "4400 0.5279855646368545 4.266495943069458\n",
      "4600 0.5251125321416723 4.316037178039551\n",
      "4800 0.529441249185429 4.291702508926392\n",
      "5000 0.5328813219102719 4.328416347503662\n",
      "5200 0.5312152569751301 4.3067786693573\n",
      "5400 0.5327344898136257 4.3635780811309814\n",
      "5600 0.5365107358406571 4.381447076797485\n",
      "5800 0.539848372447757 4.369494199752808\n",
      "6000 0.5381598313702687 4.363961219787598\n",
      "6200 0.5442570496022576 4.206986904144287\n",
      "6400 0.5440724800705699 3.9845802783966064\n",
      "6600 0.5418085260645995 4.124425411224365\n",
      "6800 0.5478705605966538 4.278129816055298\n",
      "7000 0.5467431513242982 4.252992630004883\n",
      "7200 0.5487646525889094 4.195720434188843\n",
      "7400 0.5481227567913389 4.44035267829895\n",
      "7600 0.5518056468587886 4.441452741622925\n",
      "7800 0.5548929982303938 4.345317363739014\n",
      "8000 0.5494012380549225 4.276228189468384\n",
      "8200 0.5555652597331691 4.337535619735718\n",
      "8400 0.5562686978659006 4.323462009429932\n",
      "8600 0.5562898394021291 4.4396867752075195\n",
      "8800 0.5586026733232359 4.479055166244507\n",
      "9000 0.556681122794934 4.425585508346558\n",
      "9200 0.5575787065761035 4.435773849487305\n",
      "9400 0.5613060267368294 4.414567708969116\n",
      "9600 0.5588072769067458 4.490977764129639\n",
      "9800 0.5567023159947087 4.483815670013428\n",
      "10000 0.5572823846264101 4.47385835647583\n",
      "10200 0.5560718406286725 4.49878454208374\n",
      "10400 0.5630428050746199 4.171172142028809\n",
      "10600 0.5609622373298977 4.349801540374756\n",
      "10800 0.5587849807845963 4.383197784423828\n",
      "11000 0.5626857785047517 4.3385725021362305\n",
      "11200 0.5563852561049341 4.30591344833374\n",
      "11400 0.5631507154890654 4.299479007720947\n",
      "11600 0.5665804868659965 4.269192695617676\n",
      "11800 0.5647903670903126 4.272526979446411\n",
      "12000 0.5569968073318796 4.349406480789185\n",
      "12200 0.5660979220242353 4.2858192920684814\n",
      "12400 0.5661800097695916 3.9272892475128174\n",
      "12600 0.5628693914163507 4.252553462982178\n",
      "12800 0.5679703044019582 4.257833242416382\n",
      "13000 0.5689286868431269 4.233979940414429\n",
      "13200 0.558609604655367 4.218689918518066\n",
      "13400 0.5633143695581079 4.295423269271851\n",
      "13600 0.5636438458879479 4.334255218505859\n",
      "13800 0.5688194491390186 4.3696348667144775\n",
      "14000 0.5676798640568184 4.3015663623809814\n",
      "14200 0.5698252380040177 4.293234348297119\n",
      "14400 0.5633041731900797 4.31606912612915\n",
      "14600 0.5699652717320466 4.294239282608032\n",
      "14800 0.5686242371704111 4.296207904815674\n",
      "15000 0.5675527510725041 4.28713059425354\n",
      "15200 0.5704495757214583 4.298204183578491\n",
      "15400 0.5704483941431335 4.250523805618286\n",
      "15600 0.1867795601159663 4.239796876907349\n",
      "15800 0.5688223038064281 4.322045087814331\n",
      "16000 0.5709471540636346 4.298362731933594\n",
      "16200 0.5664606997829964 4.260652542114258\n",
      "16400 0.570205541642714 4.251030683517456\n",
      "16600 0.5719030129410436 4.264523506164551\n",
      "16800 0.5522803756970913 4.312129259109497\n",
      "17000 0.5735519192991437 4.3378801345825195\n",
      "17200 0.5728556141471257 4.289771556854248\n",
      "17400 0.5649870042018574 4.324163436889648\n",
      "17600 0.5690610335705489 4.2574450969696045\n",
      "17800 0.5720464364081508 4.250025510787964\n",
      "18000 0.5551167726438067 4.215888261795044\n",
      "18200 0.5702501852992019 3.9470787048339844\n",
      "18400 0.5580802448370972 4.323202133178711\n",
      "18600 0.5694124517056858 4.28954553604126\n",
      "18800 0.5721941531521273 4.275561809539795\n",
      "19000 0.5758657647134469 4.2704010009765625\n",
      "19200 0.5693535364784237 4.297321557998657\n",
      "19400 0.568442791187809 4.322963714599609\n",
      "19600 0.5766685398527145 4.310249090194702\n",
      "19800 0.5754489869113539 4.271829128265381\n",
      "20000 0.5761782124978687 4.348574876785278\n",
      "20200 0.5757495926511815 4.361695289611816\n",
      "20400 -0.01367326300884053 4.361798524856567\n",
      "20600 -0.006381476189278201 4.3860228061676025\n",
      "20800 -0.00566261900708076 4.33478856086731\n",
      "21000 -0.008442317371573749 4.284304141998291\n",
      "21200 -0.006670008574498176 4.2824366092681885\n",
      "21400 -0.005829737660983714 4.303375959396362\n",
      "21600 -0.0051131488369020595 4.278785467147827\n",
      "21800 -0.0045440786459869376 4.308557510375977\n",
      "22000 -0.004147809580377304 4.325958013534546\n",
      "22200 -0.0038918334158088985 4.2999162673950195\n",
      "22400 -0.0037355694341318912 4.249406814575195\n",
      "22600 -0.0036371857908814587 4.263435363769531\n",
      "22800 -0.003575246092198725 4.320199012756348\n",
      "23000 -0.003532970141426617 4.310597896575928\n",
      "23200 -0.0035021966424779727 4.322120428085327\n",
      "23400 -0.0034826197374857593 4.358854293823242\n",
      "23600 -0.0034621587735215176 4.3173911571502686\n",
      "23800 -0.003448520942997968 4.312366485595703\n",
      "24000 -0.003445115530819276 4.387847900390625\n",
      "24200 -0.0034483191407586045 4.400843620300293\n",
      "24400 -0.0034587021525693817 4.34446382522583\n",
      "24600 -0.0034761551535948954 4.3536248207092285\n",
      "24800 -0.0035028514594703797 4.312108278274536\n",
      "25000 -0.0035410793381611683 4.375825881958008\n",
      "25200 -0.0035821087020981857 4.310967206954956\n",
      "25400 -0.0036219147208278457 4.372299909591675\n",
      "25600 -0.0036086295093065157 4.348411798477173\n",
      "25800 -0.003634153315930533 4.388050079345703\n",
      "26000 -0.00369885376532908 4.248272180557251\n",
      "26200 -0.0038960384260045 4.3029890060424805\n",
      "26400 -0.004315205339923578 4.102550029754639\n",
      "26600 -0.004738870401697737 4.24706506729126\n",
      "26800 -0.005081062496375521 4.244603157043457\n",
      "27000 -0.005287780107455786 4.316195249557495\n",
      "27200 -0.005518572826419765 4.277831315994263\n",
      "27400 -0.005679542408234539 4.328267335891724\n",
      "27600 -0.005656431399112538 4.268165111541748\n",
      "27800 -0.005451774275736954 4.4248106479644775\n",
      "28000 -0.005499888752731774 4.362967014312744\n",
      "28200 -0.005556674636921999 4.164835214614868\n",
      "28400 -0.005608090812325717 4.2684996128082275\n",
      "28600 -0.005585710053806592 4.254227161407471\n",
      "28800 -0.005536066413055707 4.237282752990723\n",
      "29000 -0.005468559461857092 4.2283775806427\n",
      "29200 -0.005595891636263062 4.29573655128479\n",
      "29400 -0.005637833582787577 4.359196186065674\n",
      "29600 -0.005626511884725627 4.234764814376831\n",
      "29800 -0.0056557734970483775 4.1341986656188965\n",
      "30000 -0.005732822385037473 4.168181896209717\n",
      "30200 -0.005786751561556391 4.336109161376953\n",
      "30400 -0.005866521691150428 4.314705848693848\n",
      "30600 -0.00594625086801881 4.3519697189331055\n",
      "30800 -0.005973805384275055 4.351130723953247\n",
      "31000 -0.006008246651591387 4.391641139984131\n",
      "31200 -0.006040485781975358 4.347557306289673\n",
      "31400 -0.006103232641437639 4.324134588241577\n",
      "31600 -0.006099051890930296 4.289399862289429\n",
      "31800 -0.0059916686470018465 4.2694830894470215\n",
      "32000 -0.008550003475198187 4.309391260147095\n",
      "32200 -0.004991056840318642 4.284247636795044\n",
      "32400 -0.004615841202423532 4.264643669128418\n",
      "32600 -0.004513182172724672 4.25428032875061\n",
      "32800 -0.004484608125603362 4.267735958099365\n",
      "33000 -0.004473441786833732 4.190362215042114\n",
      "33200 -0.004465612311781886 4.0095460414886475\n",
      "33400 -0.004451299827465949 4.303292512893677\n",
      "33600 -0.004436515148217968 4.3694658279418945\n",
      "33800 -0.004414001355816488 4.169699430465698\n",
      "34000 -0.004390793630493439 4.254446983337402\n",
      "34200 -0.004362425223078725 4.289735794067383\n",
      "34400 -0.00433221845902132 4.286921977996826\n",
      "34600 -0.004307107413970446 4.288675546646118\n",
      "34800 -0.0042849101612303375 4.06442928314209\n",
      "35000 -0.004289205171866151 4.231281995773315\n",
      "35200 -0.004254767438802043 4.23158860206604\n",
      "35400 -0.004216003796216228 4.284491777420044\n",
      "35600 -0.004186676935488576 4.236030340194702\n",
      "35800 -0.004156599600291688 4.307117462158203\n",
      "36000 -0.004127635839658694 4.275189161300659\n",
      "36200 -0.0041095208313497444 4.352127313613892\n",
      "36400 -0.004099268169537096 4.160341262817383\n",
      "36600 -0.004111242489464928 4.337381601333618\n",
      "36800 -0.004139435918877776 4.261940956115723\n",
      "37000 -0.004168370669649812 4.203649282455444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37200 -0.004200867996834834 4.279984951019287\n",
      "37400 -0.004246886035821795 4.238586187362671\n",
      "37600 -0.004305177078162359 4.26428484916687\n",
      "37800 -0.00438467512306806 4.21666693687439\n",
      "38000 -0.004454042447899154 4.223207950592041\n",
      "38200 -0.003965192899544112 4.203306436538696\n",
      "38400 -0.0035356196998854728 4.2994606494903564\n",
      "38600 -0.0033631911496503353 4.443097114562988\n",
      "38800 -0.003119069615673098 4.492403268814087\n",
      "39000 -0.0031091038253858727 4.406599760055542\n",
      "39200 -0.0028920265371004117 4.170337677001953\n",
      "39400 -0.0027310094046093525 4.176195383071899\n",
      "39600 -0.0026020177985810466 3.830467462539673\n",
      "39800 -0.0025042284165708306 4.251819610595703\n",
      "40000 -0.0023424764618589222 4.32561182975769\n",
      "40200 -0.00224239118986036 4.3517491817474365\n",
      "40400 -0.002267932977826348 4.337388753890991\n",
      "40600 -0.0021376691224941347 4.220123052597046\n",
      "40800 -0.002151308103085503 4.27408766746521\n",
      "41000 0.0037514168176888685 4.239209175109863\n",
      "41200 2.9715300997601446e-08 4.177343130111694\n",
      "41400 -1.4981312496331611e-05 4.364728689193726\n",
      "41600 -0.0005293020880734467 4.345008611679077\n",
      "41800 -0.0005293020880734467 4.290256023406982\n",
      "42000 -0.0005293020880734467 4.22691535949707\n",
      "42200 -0.0005293020880734467 4.302544116973877\n",
      "42400 -0.0005293020880734467 4.197760820388794\n",
      "42600 -0.0005293020880734467 4.428144216537476\n",
      "42800 -0.0005293020880734467 4.30453085899353\n",
      "43000 -0.0005293020880734467 4.338074684143066\n",
      "43200 -0.0005293020880734467 4.285693407058716\n",
      "43400 -0.0005293020880734467 4.2532289028167725\n",
      "43600 -0.0005293020880734467 4.232431411743164\n",
      "43800 -0.0005293020880734467 4.258790493011475\n",
      "44000 -0.0005293020880734467 4.3309125900268555\n",
      "44200 -0.0005293020880734467 4.3469483852386475\n",
      "44400 -0.0005293020880734467 4.333598375320435\n",
      "44600 -0.0005293020880734467 4.260319471359253\n",
      "44800 -0.0005293020880734467 4.387530565261841\n",
      "45000 -0.0005293020880734467 4.3509910106658936\n",
      "45200 -0.0005293020880734467 4.361598491668701\n",
      "45400 -0.0005293020880734467 4.192105770111084\n",
      "45600 -0.0005293020880734467 4.321882486343384\n",
      "45800 -0.0005293020880734467 4.322252988815308\n",
      "46000 -0.0005293020880734467 4.34260630607605\n",
      "46200 -0.0005293020880734467 4.3733320236206055\n",
      "46400 -0.0005293020880734467 4.3900368213653564\n",
      "46600 -0.0005293020880734467 4.303647518157959\n",
      "46800 -0.0005293020880734467 4.300814867019653\n",
      "47000 -0.0005293020880734467 4.3663928508758545\n",
      "47200 -0.0005293020880734467 4.376911401748657\n",
      "47400 -0.0005293020880734467 4.3706018924713135\n",
      "47600 -0.0005293020880734467 4.381592035293579\n",
      "47800 -0.0005293020880734467 4.420785903930664\n",
      "48000 -0.0005293020880734467 4.360885858535767\n",
      "48200 -0.0005293020880734467 4.340383291244507\n",
      "48400 -0.0005293020880734467 4.282854080200195\n",
      "48600 -0.0005293020880734467 4.245380401611328\n",
      "48800 -0.0005293020880734467 4.356543302536011\n",
      "49000 -0.0005293020880734467 4.49184775352478\n",
      "49200 -0.0005293020880734467 4.374902725219727\n",
      "49400 -0.0005293020880734467 4.201533555984497\n",
      "49600 -0.0005293020880734467 4.280277490615845\n",
      "49800 -0.0005293020880734467 4.365067720413208\n",
      "50000 -0.0005293020880734467 4.383360862731934\n",
      "50200 -0.0005293020880734467 4.280926942825317\n",
      "50400 -0.0005293020880734467 4.201542139053345\n",
      "50600 -0.0005293020880734467 3.9375696182250977\n",
      "50800 -0.0005293020880734467 4.292510271072388\n",
      "51000 -0.0005293020880734467 4.438797473907471\n",
      "51200 -0.0005293020880734467 4.463624715805054\n",
      "51400 -0.0005293020880734467 4.361568927764893\n",
      "51600 -0.0005293020880734467 4.212896108627319\n",
      "51800 -0.0005293020880734467 4.067038059234619\n",
      "52000 -0.0005293020880734467 4.267171859741211\n",
      "52200 -0.0005293020880734467 4.245538711547852\n",
      "52400 -0.0005293020880734467 4.26313591003418\n",
      "52600 -0.0005293020880734467 4.322263479232788\n",
      "52800 -0.0005293020880734467 4.2547149658203125\n",
      "53000 -0.0005293020880734467 4.352345943450928\n",
      "53200 -0.0005293020880734467 4.419089078903198\n",
      "53400 -0.0005293020880734467 4.35711145401001\n",
      "53600 -0.0005293020880734467 4.452784299850464\n",
      "53800 -0.0005293020880734467 4.416218042373657\n",
      "54000 -0.0005293020880734467 4.402881383895874\n",
      "54200 -0.0005293020880734467 4.3092200756073\n",
      "54400 -0.0005293020880734467 4.427157402038574\n",
      "54600 -0.0005293020880734467 4.403437614440918\n",
      "54800 -0.0005293020880734467 4.339678764343262\n",
      "55000 -0.0005293020880734467 4.349690914154053\n",
      "55200 -0.0005293020880734467 4.431365013122559\n",
      "55400 -0.0005293020880734467 4.063539266586304\n",
      "55600 -0.0005293020880734467 4.2711122035980225\n",
      "55800 -0.0005293020880734467 4.162106275558472\n",
      "56000 -0.0005293020880734467 4.268465995788574\n",
      "56200 -0.0005293020880734467 4.295506477355957\n",
      "56400 -0.0005293020880734467 4.262741804122925\n",
      "56600 -0.0005293020880734467 4.261211633682251\n",
      "56800 -0.0005293020880734467 4.309043884277344\n",
      "57000 -0.0005293020880734467 4.301849603652954\n",
      "57200 -0.0005293020880734467 4.290429592132568\n",
      "57400 -0.0005293020880734467 4.399497032165527\n",
      "57600 -0.0005293020880734467 4.375040531158447\n",
      "57800 -0.0005293020880734467 4.3637354373931885\n",
      "58000 -0.0005293020880734467 4.37988805770874\n",
      "58200 -0.0005293020880734467 4.285446405410767\n",
      "58400 -0.0005293020880734467 4.294902086257935\n",
      "58600 -0.0005293020880734467 4.250890493392944\n",
      "58800 -0.0005293020880734467 4.273440361022949\n",
      "59000 -0.0005293020880734467 4.418078899383545\n",
      "59200 -0.0005293020880734467 4.403206825256348\n",
      "59400 -0.0005293020880734467 4.394447565078735\n",
      "59600 -0.0005293020880734467 4.433238744735718\n",
      "59800 -0.0005293020880734467 4.350913047790527\n",
      "60000 -0.0005293020880734467 4.290760040283203\n",
      "60200 -0.0005293020880734467 4.295689582824707\n",
      "60400 -0.0005293020880734467 4.294192552566528\n",
      "60600 -0.0005293020880734467 4.3202900886535645\n",
      "60800 -0.0005293020880734467 4.271114110946655\n",
      "61000 -0.0005293020880734467 4.29180121421814\n",
      "61200 -0.0005293020880734467 4.2730841636657715\n",
      "61400 -0.0005293020880734467 4.363117218017578\n",
      "61600 -0.0005293020880734467 4.351716995239258\n",
      "61800 -0.0005293020880734467 4.320488691329956\n",
      "62000 -0.0005293020880734467 4.33031964302063\n",
      "62200 -0.0005293020880734467 4.349406003952026\n",
      "62400 -0.0005293020880734467 4.395766496658325\n",
      "62600 -0.0005293020880734467 4.344271898269653\n",
      "62800 -0.0005293020880734467 4.33297872543335\n",
      "63000 -0.0005293020880734467 4.345166206359863\n",
      "63200 -0.0005293020880734467 4.340513467788696\n",
      "63400 -0.0005293020880734467 4.359820365905762\n"
     ]
    }
   ],
   "source": [
    "mll = VariationalELBO(likelihood, model, num_data=train_V_ref.shape[0])\n",
    "#mll = VariationalELBO(likelihood, model, num_data=N)\n",
    "#epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss, pred = mll(output, train_V_ref, train_S_E, train_S_I)\n",
    "    #epochs_iter.set_postfix(loss=loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if count%200 == 0:\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        test_output = model(train_x)\n",
    "        test_loss, test_pred = mll(test_output, test_V_ref, test_S_E, test_S_I)\n",
    "        \n",
    "        \n",
    "        test_score = metrics.explained_variance_score(y_true=test_V_ref.cpu().detach().numpy(),\n",
    "                                                      y_pred=test_pred.cpu().detach().numpy(),\n",
    "                                                      multioutput='uniform_average')\n",
    "        print(count, test_score, time.time() - s)\n",
    "        s = time.time()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T18:07:53.852897Z",
     "start_time": "2020-12-09T18:07:53.842997Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T20:08:23.672Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(test_V_ref.cpu().detach().numpy()[1000:4000])\n",
    "plt.plot(test_pred.cpu().detach().numpy()[1000:4000]-61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T20:08:23.673Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "F = test_output.mean.T + torch.sqrt(test_output.variance.T)\n",
    "plt.plot(F[6].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
