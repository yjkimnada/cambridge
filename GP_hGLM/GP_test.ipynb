{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T23:58:12.338317Z",
     "start_time": "2020-12-09T23:58:11.622059Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "\n",
    "from gpytorch.likelihoods import _GaussianLikelihoodBase\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from gpytorch.distributions import MultivariateNormal, base_distributions\n",
    "from gpytorch.lazy import ZeroLazyTensor\n",
    "from gpytorch.utils.warnings import GPInputWarning\n",
    "from gpytorch.likelihoods.likelihood import Likelihood\n",
    "from gpytorch.likelihoods.noise_models import FixedGaussianNoise, HomoskedasticNoise, Noise\n",
    "from typing import Any, Optional\n",
    "from gpytorch.mlls._approximate_mll import _ApproximateMarginalLogLikelihood\n",
    "\n",
    "from gpytorch.constraints import GreaterThan\n",
    "from gpytorch.distributions import base_distributions\n",
    "from gpytorch.functions import add_diag\n",
    "from gpytorch.lazy import (\n",
    "    BlockDiagLazyTensor,\n",
    "    DiagLazyTensor,\n",
    "    KroneckerProductLazyTensor,\n",
    "    MatmulLazyTensor,\n",
    "    RootLazyTensor,\n",
    "    lazify,\n",
    ")\n",
    "from gpytorch.likelihoods import Likelihood, _GaussianLikelihoodBase\n",
    "from gpytorch.utils.warnings import OldVersionWarning\n",
    "from gpytorch.likelihoods.noise_models import MultitaskHomoskedasticNoise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T23:58:12.342895Z",
     "start_time": "2020-12-09T23:58:12.339394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "train_T = 65000\n",
    "test_T = 15000\n",
    "N = 200\n",
    "M = 3\n",
    "batch_size = 1500\n",
    "\n",
    "C_den = torch.zeros(5,5)\n",
    "C_den[0,1:] = 1\n",
    "\n",
    "sub_no = C_den.shape[0]\n",
    "num_tasks = sub_no * 2\n",
    "\n",
    "print(torch.Size([num_tasks]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T23:58:12.378247Z",
     "start_time": "2020-12-09T23:58:12.344062Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, num_tasks, M, N):\n",
    "        # Let's use a different set of inducing points for each task\n",
    "        inducing_points = torch.rand(num_tasks, M, 1) * N\n",
    "\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([num_tasks])\n",
    "        )\n",
    "\n",
    "        variational_strategy = gpytorch.variational.IndependentMultitaskVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks=num_tasks,\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_tasks]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_tasks])),\n",
    "            batch_shape=torch.Size([num_tasks])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class CustomLikelihood( _GaussianLikelihoodBase):\n",
    "    def __init__(self, C_den, sub_no, N, num_tasks,\n",
    "        rank=0,\n",
    "        task_correlation_prior=None,\n",
    "        batch_shape=torch.Size(),\n",
    "        noise_prior=None,\n",
    "        noise_constraint=None):\n",
    "        if noise_constraint is None:\n",
    "            noise_constraint = GreaterThan(1e-4)\n",
    "\n",
    "        noise_covar = MultitaskHomoskedasticNoise(\n",
    "            num_tasks=num_tasks, noise_prior=noise_prior, noise_constraint=noise_constraint, batch_shape=batch_shape\n",
    "        )\n",
    "        super().__init__(noise_covar=noise_covar)\n",
    "        if rank != 0:\n",
    "            if rank > num_tasks:\n",
    "                raise ValueError(f\"Cannot have rank ({rank}) greater than num_tasks ({num_tasks})\")\n",
    "            tidcs = torch.tril_indices(num_tasks, rank, dtype=torch.long)\n",
    "            self.tidcs = tidcs[:, 1:]  # (1, 1) must be 1.0, no need to parameterize this\n",
    "            task_noise_corr = torch.randn(*batch_shape, self.tidcs.size(-1))\n",
    "            self.register_parameter(\"task_noise_corr\", torch.nn.Parameter(task_noise_corr))\n",
    "            if task_correlation_prior is not None:\n",
    "                self.register_prior(\n",
    "                    \"MultitaskErrorCorrelationPrior\", task_correlation_prior, lambda: self._eval_corr_matrix\n",
    "                )\n",
    "        elif task_correlation_prior is not None:\n",
    "            raise ValueError(\"Can only specify task_correlation_prior if rank>0\")\n",
    "        self.num_tasks = num_tasks\n",
    "        self.rank = rank\n",
    "                \n",
    "        self.C_den = C_den\n",
    "        self.sub_no = sub_no\n",
    "        self.N = N\n",
    "        \n",
    "        self.decay = nn.Parameter(torch.ones(self.sub_no*2) , requires_grad=True)\n",
    "        self.shift = nn.Parameter(torch.zeros(self.sub_no*2) , requires_grad=True)\n",
    "        self.scale = nn.Parameter(torch.ones(self.sub_no*2) , requires_grad=True)\n",
    "        \n",
    "        # Between Subunit Parameters\n",
    "        self.W_log = nn.Parameter(torch.zeros(self.sub_no) , requires_grad=True) # POSITIVE\n",
    "\n",
    "        ### Subunit Output Parameters ###\n",
    "        self.V_o = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "        self.Theta = nn.Parameter(torch.zeros(self.sub_no), requires_grad=True)\n",
    "    \n",
    "    @property\n",
    "    def noise(self):\n",
    "        return self.raw_noise_constraint.transform(self.raw_noise)\n",
    "\n",
    "    @noise.setter\n",
    "    def noise(self, value):\n",
    "        self._set_noise(value)\n",
    "\n",
    "    def _set_noise(self, value):\n",
    "        if not torch.is_tensor(value):\n",
    "            value = torch.as_tensor(value).to(self.raw_noise)\n",
    "        self.initialize(raw_noise=self.raw_noise_constraint.inverse_transform(value))\n",
    "\n",
    "    def _shaped_noise_covar(self, base_shape, *params):\n",
    "        if len(base_shape) >= 2:\n",
    "            *batch_shape, n, _ = base_shape\n",
    "        else:\n",
    "            *batch_shape, n = base_shape\n",
    "\n",
    "        # compute the noise covariance\n",
    "        if len(params) > 0:\n",
    "            shape = None\n",
    "        else:\n",
    "            shape = base_shape if len(base_shape) == 1 else base_shape[:-1]\n",
    "        noise_covar = self.noise_covar(*params, shape=shape)\n",
    "\n",
    "        if self.rank > 0:\n",
    "            # if rank > 0, compute the task correlation matrix\n",
    "            # TODO: This is inefficient, change repeat so it can repeat LazyTensors w/ multiple batch dimensions\n",
    "            task_corr = self._eval_corr_matrix()\n",
    "            exp_shape = torch.Size([*batch_shape, n]) + task_corr.shape[-2:]\n",
    "            task_corr_exp = lazify(task_corr.unsqueeze(-3).expand(exp_shape))\n",
    "            noise_sem = noise_covar.sqrt()\n",
    "            task_covar_blocks = MatmulLazyTensor(MatmulLazyTensor(noise_sem, task_corr_exp), noise_sem)\n",
    "        else:\n",
    "            # otherwise tasks are uncorrelated\n",
    "            task_covar_blocks = noise_covar\n",
    "\n",
    "        if len(batch_shape) == 1:\n",
    "            # TODO: Properly support general batch shapes in BlockDiagLazyTensor (no shape arithmetic)\n",
    "            tcb_eval = task_covar_blocks.evaluate()\n",
    "            task_covar = BlockDiagLazyTensor(lazify(tcb_eval), block_dim=-3)\n",
    "        else:\n",
    "            task_covar = BlockDiagLazyTensor(task_covar_blocks)\n",
    "\n",
    "        return task_covar\n",
    "        \n",
    "    def expected_log_prob(self, target: Tensor, input: MultivariateNormal, S_e, S_i, *params: Any, **kwargs: Any) -> Tensor:\n",
    "        #mean, variance = input.mean, input.variance\n",
    "        #noise = self._shaped_noise_covar(mean.shape, *params, **kwargs).diag()\n",
    "        # Potentially reshape the noise to deal with the multitask case\n",
    "        #noise = noise.view(*noise.shape[:-1], *input.event_shape)\n",
    "        \n",
    "        #sample = 7\n",
    "        #all_F = torch.zeros(self.sub_no*2, self.N).cuda()\n",
    "        #for i in range(sample):\n",
    "            #all_F = all_F + input.mean.T + torch.sqrt(input.variance.T) * torch.randn(self.sub_no*2, self.N).cuda()\n",
    "        \n",
    "        #all_F = all_F / sample\n",
    "        \n",
    "        all_F = input.mean.T\n",
    "        \n",
    "        \n",
    "        decay_dist = torch.arange(self.N).reshape(1,-1).repeat(self.sub_no*2,1).cuda() - self.shift.reshape(-1,1)\n",
    "        decay_dist_2 = decay_dist ** 2\n",
    "        decay_factor = self.scale.reshape(-1,1)**2 * torch.exp(-decay_dist_2 / self.decay.reshape(-1,1)**2)\n",
    "        \n",
    "        all_F = all_F * decay_factor\n",
    "        \n",
    "        T = S_e.shape[0]\n",
    "        \n",
    "        F_e = all_F[:self.sub_no].unsqueeze(1)\n",
    "        F_i = all_F[self.sub_no:].unsqueeze(1)\n",
    "        flip_F_e = torch.flip(F_e, [2])\n",
    "        flip_F_i = torch.flip(F_i, [2])\n",
    "        #flip_F_e = F_e\n",
    "        #flip_F_i = F_i\n",
    "        \n",
    "        pad_S_e = torch.zeros(T + self.N-1, self.sub_no).cuda()\n",
    "        pad_S_i = torch.zeros(T + self.N-1, self.sub_no).cuda()\n",
    "        pad_S_e[-T:] = pad_S_e[-T:] + S_e\n",
    "        pad_S_i[-T:] = pad_S_i[-T:] + S_i\n",
    "        pad_S_e = pad_S_e.T.unsqueeze(0)\n",
    "        pad_S_i = pad_S_i.T.unsqueeze(0)\n",
    "\n",
    "        filtered_e = F.conv1d(pad_S_e, flip_F_e, padding=0, groups=self.sub_no).squeeze(0).T\n",
    "        filtered_i = F.conv1d(pad_S_i, flip_F_i, padding=0, groups=self.sub_no).squeeze(0).T\n",
    "\n",
    "        syn_in = filtered_e + filtered_i\n",
    "\n",
    "        #----- Combine Subunits -----#\n",
    "\n",
    "        sub_out = torch.zeros(T, self.sub_no).cuda()\n",
    "        \n",
    "        for s in range(self.sub_no):\n",
    "            sub_idx = -s-1\n",
    "            leaf_idx = torch.where(self.C_den[sub_idx] == 1)[0]\n",
    "\n",
    "            if torch.numel(leaf_idx) == 0:\n",
    "                nonlin_out = torch.tanh(syn_in[:,sub_idx] + self.Theta[sub_idx]) # (T_data,) \n",
    "                sub_out[:,sub_idx] = sub_out[:,sub_idx] + nonlin_out\n",
    "            else:\n",
    "                leaf_in = sub_out[:,leaf_idx] * torch.exp(self.W_log[leaf_idx]) # (T_data,)\n",
    "                nonlin_in = syn_in[:,sub_idx] + torch.sum(leaf_in, 1) + self.Theta[sub_idx]# (T_data,)\n",
    "                nonlin_out = torch.tanh(nonlin_in)\n",
    "                sub_out[:,sub_idx] = sub_out[:,sub_idx] + nonlin_out\n",
    "        \n",
    "        final_voltage = sub_out[:,0]*torch.exp(self.W_log[0]) + self.V_o\n",
    "\n",
    "        #res = (target - final_voltage) ** 2\n",
    "        #res = res.mul(-0.5)\n",
    "        res = torch.var(target - final_voltage)\n",
    "        \n",
    "        return res, final_voltage, all_F\n",
    "    \n",
    "class VariationalELBO(_ApproximateMarginalLogLikelihood):\n",
    "    def _log_likelihood_term(self, variational_dist_f, target, S_e, S_i, **kwargs):\n",
    "        error, pred, all_F = self.likelihood.expected_log_prob(target, variational_dist_f, S_e, S_i, **kwargs)\n",
    "        \n",
    "        return error.sum(-1), pred, all_F\n",
    "\n",
    "    def forward(self, approximate_dist_f, target, S_e, S_i, **kwargs):\n",
    "        r\"\"\"\n",
    "        Computes the Variational ELBO given :math:`q(\\mathbf f)` and `\\mathbf y`.\n",
    "        Calling this function will call the likelihood's `expected_log_prob` function.\n",
    "        Args:\n",
    "            :attr:`approximate_dist_f` (:obj:`gpytorch.distributions.MultivariateNormal`):\n",
    "                :math:`q(\\mathbf f)` the outputs of the latent function (the :obj:`gpytorch.models.ApproximateGP`)\n",
    "            :attr:`target` (`torch.Tensor`):\n",
    "                :math:`\\mathbf y` The target values\n",
    "            :attr:`**kwargs`:\n",
    "                Additional arguments passed to the likelihood's `expected_log_prob` function.\n",
    "        \"\"\"\n",
    "        # Get likelihood term and KL term\n",
    "        num_batch = approximate_dist_f.event_shape[0]\n",
    "        log_likelihood, pred, all_F = self._log_likelihood_term(approximate_dist_f, target, S_e, S_i,**kwargs)\n",
    "        log_likelihood = log_likelihood.div(num_batch)\n",
    "        \n",
    "        kl_divergence = self.model.variational_strategy.kl_divergence().div(self.num_data / self.beta)\n",
    "\n",
    "        # Add any additional registered loss terms\n",
    "        added_loss = torch.zeros_like(log_likelihood)\n",
    "        had_added_losses = False\n",
    "        for added_loss_term in self.model.added_loss_terms():\n",
    "            added_loss.add_(added_loss_term.loss())\n",
    "            had_added_losses = True\n",
    "\n",
    "        # Log prior term\n",
    "        log_prior = torch.zeros_like(log_likelihood)\n",
    "        for _, prior, closure, _ in self.named_priors():\n",
    "            log_prior.add_(prior.log_prob(closure()).sum().div(self.num_data))\n",
    "\n",
    "        if self.combine_terms:\n",
    "            return log_likelihood - kl_divergence + log_prior - added_loss , pred, all_F\n",
    "            #return log_likelihood , pred\n",
    "        else:\n",
    "            if had_added_losses:\n",
    "                return log_likelihood, kl_divergence, log_prior.div(self.num_data), added_loss\n",
    "            else:\n",
    "                return log_likelihood, kl_divergence, log_prior.div(self.num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T23:58:12.387106Z",
     "start_time": "2020-12-09T23:58:12.379449Z"
    }
   },
   "outputs": [],
   "source": [
    "Ensyn = torch.tensor([0, 106, 213, 211, 99])\n",
    "Insyn = torch.tensor([1, 22, 36, 42, 19])\n",
    "\n",
    "E_no = torch.sum(Ensyn)\n",
    "I_no = torch.sum(Insyn)\n",
    "\n",
    "C_syn_e = torch.zeros(sub_no, E_no)\n",
    "C_syn_i = torch.zeros(sub_no, I_no)\n",
    "\n",
    "E_count = 0\n",
    "for s in range(sub_no):\n",
    "    C_syn_e[s,E_count:E_count+Ensyn[s]] = 1\n",
    "    E_count += Ensyn[s]\n",
    "\n",
    "I_count = 0\n",
    "for s in range(sub_no):\n",
    "    C_syn_i[s,I_count:I_count+Insyn[s]] = 1\n",
    "    I_count += Insyn[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T23:58:14.213753Z",
     "start_time": "2020-12-09T23:58:12.388077Z"
    }
   },
   "outputs": [],
   "source": [
    "V_ref = np.load(\"/media/hdd01/sklee/L23_inputs/vdata_NMDA_ApN0.5_13_Adend_r0_o2_i2_g_b4.npy\").flatten()\n",
    "\n",
    "train_V_ref = V_ref[:train_T]\n",
    "test_V_ref = V_ref[train_T:train_T+test_T]\n",
    "test_V_ref = torch.from_numpy(test_V_ref).cuda()\n",
    "train_V_ref = torch.from_numpy(train_V_ref).cuda()\n",
    "\n",
    "raw_E_neural = np.load(\"/media/hdd01/sklee/L23_inputs/Espikes_NMDA_ApN0.5_13_Adend_r0_o2_i2_g_b4_neural.npy\")\n",
    "raw_I_neural = np.load(\"/media/hdd01/sklee/L23_inputs/Ispikes_NMDA_ApN0.5_13_Adend_r0_o2_i2_g_b4_neural.npy\")\n",
    "\n",
    "E_neural = torch.matmul(torch.from_numpy(raw_E_neural).double(), C_syn_e.T.double())\n",
    "I_neural = torch.matmul(torch.from_numpy(raw_I_neural).double(), C_syn_i.T.double())\n",
    "\n",
    "train_S_E = E_neural[:train_T].cuda()\n",
    "train_S_I = I_neural[:train_T].cuda()\n",
    "test_S_E = E_neural[train_T:train_T+test_T].double().cuda()\n",
    "test_S_I = I_neural[train_T:train_T+test_T].double().cuda()\n",
    "\n",
    "repeat_no = 1\n",
    "batch_no = (train_V_ref.shape[0] - batch_size) * repeat_no\n",
    "train_idx = np.empty((repeat_no, train_V_ref.shape[0] - batch_size))\n",
    "for i in range(repeat_no):\n",
    "    part_idx = np.arange(train_V_ref.shape[0] - batch_size)\n",
    "    np.random.shuffle(part_idx)\n",
    "    train_idx[i] = part_idx\n",
    "train_idx = train_idx.flatten()\n",
    "train_idx = torch.from_numpy(train_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T23:58:14.223653Z",
     "start_time": "2020-12-09T23:58:14.215294Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "model = MultitaskGPModel(num_tasks, M, N)\n",
    "likelihood = CustomLikelihood(C_den.cuda(), sub_no, N, num_tasks)\n",
    "\n",
    "num_epochs = 10000\n",
    "model.cuda().train()\n",
    "likelihood.cuda().train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr = 0.005)\n",
    "\n",
    "\n",
    "train_x = torch.arange(N).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T00:56:36.728328Z",
     "start_time": "2020-12-09T23:58:14.224775Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.003080712774472638 0.4412097930908203\n",
      "200 0.23514508342263984 4.2708213329315186\n",
      "400 0.341509436143987 8.019817113876343\n",
      "600 0.41215724532056797 11.851905107498169\n",
      "800 0.4731173535387656 15.784694194793701\n",
      "1000 0.5494689079348027 19.71460771560669\n",
      "1200 0.6865983925819298 23.59830117225647\n",
      "1400 0.724673804181792 27.44576334953308\n",
      "1600 0.7493481104035609 31.318103790283203\n",
      "1800 0.7688486860949013 35.22142744064331\n",
      "2000 0.7842631747991538 39.23146605491638\n",
      "2200 0.7967341971002974 43.491079568862915\n",
      "2400 0.8086193523341604 47.93841195106506\n",
      "2600 0.8174128413606174 52.472240924835205\n",
      "2800 0.8247213199265772 57.54390239715576\n",
      "3000 0.8314447955772564 62.778528690338135\n",
      "3200 0.8361675422154078 68.51134586334229\n",
      "3400 0.8411088236290329 74.62092590332031\n",
      "3600 0.8441039465536282 81.09566831588745\n",
      "3800 0.8484108353535489 88.12125825881958\n",
      "4000 0.8512850767598368 95.12865209579468\n",
      "4200 0.8535374688421848 102.16551208496094\n",
      "4400 0.8557347474124556 109.14147615432739\n",
      "4600 0.857318659341951 116.14872741699219\n",
      "4800 0.8595441484869601 123.17240405082703\n",
      "5000 0.8612539318765269 130.17811560630798\n",
      "5200 0.863240819822954 137.18903636932373\n",
      "5400 0.8641860808583327 144.3038854598999\n",
      "5600 0.8658379698556653 151.45731925964355\n",
      "5800 0.8672659934041047 158.4834668636322\n",
      "6000 0.868554024877715 165.55526494979858\n",
      "6200 0.8699977486854351 172.7174642086029\n",
      "6400 0.8712118554500887 179.7822859287262\n",
      "6600 0.8805203789748715 186.76642417907715\n",
      "6800 0.8818302957954826 193.8162169456482\n",
      "7000 0.8823325462086189 200.80641198158264\n",
      "7200 0.8838064802119212 207.91590666770935\n",
      "7400 0.8852466535183537 214.98998427391052\n",
      "7600 0.8860491864591986 221.9842553138733\n",
      "7800 0.8862510941728589 229.08064889907837\n",
      "8000 0.8864745006055754 236.18160152435303\n",
      "8200 0.8877018931339652 243.2760989665985\n",
      "8400 0.8880612309776189 250.38790917396545\n",
      "8600 0.8888840533368086 257.47881293296814\n",
      "8800 0.88855732559678 264.5463373661041\n",
      "9000 0.8903607952194893 271.6789081096649\n",
      "9200 0.8901904702922518 278.8100142478943\n",
      "9400 0.8909722244439762 285.79289841651917\n",
      "9600 0.8911675816430664 292.87474822998047\n",
      "9800 0.8918483784732663 299.9375283718109\n",
      "10000 0.8921395180044346 307.08308005332947\n",
      "10200 0.8929981730319458 314.18094658851624\n",
      "10400 0.8930865856088206 321.3165850639343\n",
      "10600 0.8933184747043511 328.3084831237793\n",
      "10800 0.8938208096894175 335.3562138080597\n",
      "11000 0.8941565468899476 342.50326204299927\n",
      "11200 0.8946292987545815 349.58761191368103\n",
      "11400 0.8943422214161451 356.75418734550476\n",
      "11600 0.8953526668375968 363.80622935295105\n",
      "11800 0.8954265419514249 370.9581594467163\n",
      "12000 0.8956386508847107 377.96602845191956\n",
      "12200 0.8947938239830453 385.0251817703247\n",
      "12400 0.8959753825780319 392.11874413490295\n",
      "12600 0.8958006168732584 399.1323609352112\n",
      "12800 0.8966819267410546 406.22274684906006\n",
      "13000 0.8970216146102687 413.3308265209198\n",
      "13200 0.8969586101623913 420.504513502121\n",
      "13400 0.8973301221530114 427.65046977996826\n",
      "13600 0.8969688431045449 434.7682423591614\n",
      "13800 0.8973908832623836 441.8991332054138\n",
      "14000 0.8977612045939373 449.00659346580505\n",
      "14200 0.8976079661609128 456.12197494506836\n",
      "14400 0.8970994624805011 463.2087781429291\n",
      "14600 0.8977849814838934 470.2308142185211\n",
      "14800 0.8981269581967247 477.34483432769775\n",
      "15000 0.8980401363275986 484.4491937160492\n",
      "15200 0.8978970359533827 491.467542886734\n",
      "15400 0.8982400005085026 498.5548198223114\n",
      "15600 0.8987846934873076 505.60746908187866\n",
      "15800 0.8975212494734752 512.75705742836\n",
      "16000 0.8982582109366656 519.9131886959076\n",
      "16200 0.8988471163577462 526.9915111064911\n",
      "16400 0.8991673271983677 534.0647089481354\n",
      "16600 0.8980243360923046 541.2070438861847\n",
      "16800 0.8991600523216905 548.2906322479248\n",
      "17000 0.8992068807815383 555.412544965744\n",
      "17200 0.8990019235723263 562.5318341255188\n",
      "17400 0.8993886899555634 569.6524293422699\n",
      "17600 0.8994002822454424 576.8022255897522\n",
      "17800 0.898916175537395 583.8924973011017\n",
      "18000 0.8993796166382958 590.9676723480225\n",
      "18200 0.899552471507681 598.088449716568\n",
      "18400 0.8995489379886135 605.0992858409882\n",
      "18600 0.8996610435938579 612.2205150127411\n",
      "18800 0.8995752391677998 619.2559835910797\n",
      "19000 0.9002433770631361 626.2899050712585\n",
      "19200 0.8996895071843531 633.434202671051\n",
      "19400 0.8992486687058439 640.4711697101593\n",
      "19600 0.9000341679865388 647.5660650730133\n",
      "19800 0.8990386009716804 654.6322014331818\n",
      "20000 0.9002242782631565 661.7739431858063\n",
      "20200 0.9004008352668057 668.8617854118347\n",
      "20400 0.9001661519195729 675.9106435775757\n",
      "20600 0.9001597349737364 683.0490670204163\n",
      "20800 0.9001841972043807 690.044664144516\n",
      "21000 0.9003438441484634 697.1429381370544\n",
      "21200 0.9003212165619932 704.2589688301086\n",
      "21400 0.9002070796498329 711.3766193389893\n",
      "21600 0.9004414934623958 718.4883770942688\n",
      "21800 0.9004527015696786 725.6080174446106\n",
      "22000 0.9005110837907332 732.7269055843353\n",
      "22200 0.9008201055892697 739.8283002376556\n",
      "22400 0.9005631592794745 747.0062894821167\n",
      "22600 0.9006055339147463 754.0923345088959\n",
      "22800 0.9005454468757936 761.2213363647461\n",
      "23000 0.9006117729657122 768.3491933345795\n",
      "23200 0.9006479097956414 775.493602514267\n",
      "23400 0.9007209969205873 782.5785412788391\n",
      "23600 0.9008166364071176 789.7339713573456\n",
      "23800 0.9008851212220563 796.8314182758331\n",
      "24000 0.9006179485423498 803.9242947101593\n",
      "24200 0.9005043995218235 810.9238948822021\n",
      "24400 0.9012596853538986 818.0882573127747\n",
      "24600 0.9008249496829374 825.1342613697052\n",
      "24800 0.9010884830444782 832.1222505569458\n",
      "25000 0.9007184921629925 839.2490944862366\n",
      "25200 0.8997806771706686 846.2648749351501\n",
      "25400 0.9013713921351634 853.4473874568939\n",
      "25600 0.9011026129884914 860.5900399684906\n",
      "25800 0.9012939374792805 867.6977314949036\n",
      "26000 0.9007785743233087 874.8455357551575\n",
      "26200 0.9011546533769352 881.8768587112427\n",
      "26400 0.9013343184445207 889.0716755390167\n",
      "26600 0.9009825345321885 896.1691129207611\n",
      "26800 0.9011104173404686 903.2951371669769\n",
      "27000 0.9010592276540718 910.3674914836884\n",
      "27200 0.9011701661105884 917.4435102939606\n",
      "27400 0.9011473926209517 924.555703163147\n",
      "27600 0.9012027085026768 931.611456155777\n",
      "27800 0.9011706253773596 938.6771318912506\n",
      "28000 0.9015408119725413 945.7286787033081\n",
      "28200 0.9009976451607539 952.8366930484772\n",
      "28400 0.9012451730077954 959.9186999797821\n",
      "28600 0.9012138154515393 967.0089180469513\n",
      "28800 0.9010298869632167 974.1273891925812\n",
      "29000 0.9011189608060313 981.2146515846252\n",
      "29200 0.9016987719947747 988.3367443084717\n",
      "29400 0.9014423522992469 995.4173147678375\n",
      "29600 0.9012371765558456 1002.452314376831\n",
      "29800 0.9013902650303098 1009.5440165996552\n",
      "30000 0.9012403901669273 1016.6237623691559\n",
      "30200 0.9014947386434535 1023.7423615455627\n",
      "30400 0.9011838547320286 1030.9495871067047\n",
      "30600 0.9017517110601083 1038.0507044792175\n",
      "30800 0.9014764649917746 1045.16734790802\n",
      "31000 0.9014754963004181 1052.2847933769226\n",
      "31200 0.901576928656967 1059.2620403766632\n",
      "31400 0.9015329296288423 1066.3546528816223\n",
      "31600 0.9015771633809448 1073.4341921806335\n",
      "31800 0.9014733256710898 1080.5804650783539\n",
      "32000 0.901575850359164 1087.6901552677155\n",
      "32200 0.9015326758231254 1094.8430752754211\n",
      "32400 0.9015778461500524 1101.8977251052856\n",
      "32600 0.9015654268167936 1108.9916813373566\n",
      "32800 0.9016063205058197 1116.0737702846527\n",
      "33000 0.9016210647530787 1123.2007460594177\n",
      "33200 0.9016658645496538 1130.390869140625\n",
      "33400 0.9017266257441331 1137.4721992015839\n",
      "33600 0.9014659331124129 1144.5969276428223\n",
      "33800 0.9017244472441573 1151.7514255046844\n",
      "34000 0.901690313152355 1158.910902261734\n",
      "34200 0.9017111917134155 1166.0954222679138\n",
      "34400 0.9015860854136781 1173.1284592151642\n",
      "34600 0.901740208507465 1180.2191541194916\n",
      "34800 0.901770961589238 1187.2004690170288\n",
      "35000 0.9019531402865737 1194.3123986721039\n",
      "35200 0.9021179950925435 1201.3819689750671\n",
      "35400 0.9019284404174952 1208.4347486495972\n",
      "35600 0.9018432459068412 1215.5325865745544\n",
      "35800 0.9016963571315706 1222.6617760658264\n",
      "36000 0.9019372387468181 1229.7616415023804\n",
      "36200 0.9018696660756318 1236.8716361522675\n",
      "36400 0.901937363115755 1243.957968711853\n",
      "36600 0.902120004280177 1251.0831043720245\n",
      "36800 0.9016326140631755 1258.0953323841095\n",
      "37000 0.9019190814475626 1265.2083163261414\n",
      "37200 0.9019339952928049 1272.2687530517578\n",
      "37400 0.901897286804148 1279.3894221782684\n",
      "37600 0.9019241580189842 1286.4757211208344\n",
      "37800 0.902116008318042 1293.6156616210938\n",
      "38000 0.9019264954035286 1300.7391736507416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38200 0.901990488839371 1307.9174027442932\n",
      "38400 0.9019779674255418 1315.0399413108826\n",
      "38600 0.9014245450109881 1322.0853753089905\n",
      "38800 0.9019141817085975 1329.1856515407562\n",
      "39000 0.9020729064523639 1336.373967885971\n",
      "39200 0.9017796783026835 1343.4567787647247\n",
      "39400 0.9020817508792041 1350.5198078155518\n",
      "39600 0.9024506319192092 1357.5659925937653\n",
      "39800 0.9018948134872715 1364.6610729694366\n",
      "40000 0.9018391249812819 1371.7313516139984\n",
      "40200 0.9020457209599009 1378.8560156822205\n",
      "40400 0.9022389920903959 1385.9289240837097\n",
      "40600 0.9020479333924997 1393.0471680164337\n",
      "40800 0.9021956742838682 1400.1688249111176\n",
      "41000 0.90198701871807 1407.30739569664\n",
      "41200 0.902075208300324 1414.3509547710419\n",
      "41400 0.9020505102364786 1421.3904497623444\n",
      "41600 0.9022520743503342 1428.4600570201874\n",
      "41800 0.9022589199938086 1435.6130542755127\n",
      "42000 0.902180959643671 1442.7406480312347\n",
      "42200 0.90207195687889 1449.8016362190247\n",
      "42400 0.9021216293943937 1456.9830462932587\n",
      "42600 0.902207191265876 1464.0836462974548\n",
      "42800 0.9025261788070151 1471.1874854564667\n",
      "43000 0.902139921913153 1478.2288446426392\n",
      "43200 0.9020671800841413 1485.275884628296\n",
      "43400 0.9026727359188459 1492.3396635055542\n",
      "43600 0.9022268599118515 1499.518752336502\n",
      "43800 0.902459042055812 1506.6786673069\n",
      "44000 0.9025805975299732 1513.7027261257172\n",
      "44200 0.9022576290419605 1520.8859195709229\n",
      "44400 0.9025109721022575 1527.985506772995\n",
      "44600 0.9022218207980182 1535.095899105072\n",
      "44800 0.902252259784414 1542.241447210312\n",
      "45000 0.9022671305749271 1549.3218307495117\n",
      "45200 0.9027619319630401 1556.3638093471527\n",
      "45400 0.9022034545433367 1563.4409546852112\n",
      "45600 0.9022890318560789 1570.534603357315\n",
      "45800 0.9021995559735865 1577.639274597168\n",
      "46000 0.9017165461841943 1584.670851945877\n",
      "46200 0.9022708537843133 1591.7940139770508\n",
      "46400 0.9022655976974057 1598.8968586921692\n",
      "46600 0.9023092992305837 1605.9388365745544\n",
      "46800 0.902439116155955 1612.989162683487\n",
      "47000 0.9027874345153052 1619.9859528541565\n",
      "47200 0.9017789982287038 1627.1158783435822\n",
      "47400 0.902322070632247 1634.3255333900452\n",
      "47600 0.902372999821766 1641.4503276348114\n",
      "47800 0.9024992289688192 1648.5235891342163\n",
      "48000 0.9025149868499815 1655.6156470775604\n",
      "48200 0.90212637603032 1662.6404252052307\n",
      "48400 0.902342389508396 1669.720972776413\n",
      "48600 0.902407997297831 1676.8794312477112\n",
      "48800 0.9024530500981403 1683.9389197826385\n",
      "49000 0.9024419760433542 1691.0691404342651\n",
      "49200 0.9026355668617443 1698.2301635742188\n",
      "49400 0.9025619078950131 1705.2407546043396\n",
      "49600 0.9024104826222555 1712.4312438964844\n",
      "49800 0.9024486870426779 1719.635727405548\n",
      "50000 0.9024202851637915 1726.8057751655579\n",
      "50200 0.9021930352293867 1733.8724591732025\n",
      "50400 0.9024580699484765 1740.9950461387634\n",
      "50600 0.9012452420176136 1748.0343816280365\n",
      "50800 0.902479372762048 1755.2070937156677\n",
      "51000 0.90241162201338 1762.2740886211395\n",
      "51200 0.9024834795193033 1769.3554396629333\n",
      "51400 0.9029303723768622 1776.3869590759277\n",
      "51600 0.9024779552112524 1783.5187752246857\n",
      "51800 0.902637853027331 1790.6236326694489\n",
      "52000 0.9024004781935727 1797.7897617816925\n",
      "52200 0.9025151808723885 1804.869728565216\n",
      "52400 0.9024145104265915 1811.9791886806488\n",
      "52600 0.9022329042131328 1819.1703436374664\n",
      "52800 0.9023000409218899 1826.333647966385\n",
      "53000 0.9023894511455074 1833.455754995346\n",
      "53200 0.9027966652381497 1840.568023443222\n",
      "53400 0.902456151487796 1847.6851449012756\n",
      "53600 0.9025431149113845 1854.6920869350433\n",
      "53800 0.9022901778159177 1861.7302460670471\n",
      "54000 0.9022639777133169 1868.7128839492798\n",
      "54200 0.9025555347487662 1875.8797278404236\n",
      "54400 0.9025325751870652 1883.033442735672\n",
      "54600 0.90254550968396 1890.188184261322\n",
      "54800 0.9026568574362439 1897.251945257187\n",
      "55000 0.902928500097565 1904.3122007846832\n",
      "55200 0.9026070358381804 1911.3794915676117\n",
      "55400 0.9026732720352207 1918.4363763332367\n",
      "55600 0.9026732967965237 1925.4470295906067\n",
      "55800 0.9022606829027568 1932.5139570236206\n",
      "56000 0.9024979588207388 1939.6230235099792\n",
      "56200 0.9026168515941518 1946.6697914600372\n",
      "56400 0.9025773911081799 1953.76456284523\n",
      "56600 0.9026049403103145 1960.8618791103363\n",
      "56800 0.9027662406144233 1967.998607158661\n",
      "57000 0.9026436181220581 1975.1367321014404\n",
      "57200 0.902083274641484 1982.1891741752625\n",
      "57400 0.9026377246090277 1989.3719277381897\n",
      "57600 0.9030157761223752 1996.4829542636871\n",
      "57800 0.9025833551834864 2003.6381258964539\n",
      "58000 0.9026508649815841 2010.764649629593\n",
      "58200 0.9026593016675077 2017.9292402267456\n",
      "58400 0.9027805175356833 2024.9919979572296\n",
      "58600 0.9027114792485432 2032.1187946796417\n",
      "58800 0.9027363383835786 2039.1786360740662\n",
      "59000 0.9028557395012654 2046.2822391986847\n",
      "59200 0.9020960811661902 2053.4679358005524\n",
      "59400 0.90275583228078 2060.5380339622498\n",
      "59600 0.9026416115881202 2067.6436388492584\n",
      "59800 0.9026388484905401 2074.7520747184753\n",
      "60000 0.902605011791088 2081.8020231723785\n",
      "60200 0.902835242176257 2088.974466562271\n",
      "60400 0.9024940697471555 2096.081832408905\n",
      "60600 0.902915016972849 2103.1512792110443\n",
      "60800 0.9025428147427514 2110.1981871128082\n",
      "61000 0.9029587724223487 2117.180698156357\n",
      "61200 0.9025884186176603 2124.256278038025\n",
      "61400 0.9022968320658303 2131.3037297725677\n",
      "61600 0.9024691313258043 2138.4617533683777\n",
      "61800 0.9027311230170169 2145.561759710312\n",
      "62000 0.9032069967336066 2152.6903717517853\n",
      "62200 0.9027621207710402 2159.7297356128693\n",
      "62400 0.9030865585343049 2166.8051121234894\n",
      "62600 0.9026754573910711 2173.9011731147766\n",
      "62800 0.9025890938972029 2181.0207583904266\n",
      "63000 0.9029526592506746 2188.1314582824707\n",
      "63200 0.9027373780356776 2195.2026917934418\n",
      "63400 0.9018673748967379 2202.330538749695\n",
      "63600 0.9027172585324337 2209.477295398712\n",
      "63800 0.9027251861330807 2216.6088650226593\n",
      "64000 0.9032612246000143 2223.748161315918\n",
      "64200 0.9027218781887685 2230.9466218948364\n",
      "64400 0.9031721980762496 2238.1325540542603\n",
      "64600 0.9028995293724003 2245.217000722885\n",
      "64800 0.9031448577497184 2252.267288684845\n",
      "65000 0.9026430955622337 2259.3271160125732\n",
      "65200 0.9027350349744852 2266.4148914813995\n",
      "65400 0.9024531619371111 2273.542151451111\n",
      "65600 0.9030991600762134 2280.626459121704\n",
      "65800 0.9027740650315226 2287.7430868148804\n",
      "66000 0.9028427458408825 2294.767144203186\n",
      "66200 0.9027372025389648 2301.836961746216\n",
      "66400 0.9024524800188145 2308.9309911727905\n",
      "66600 0.9026591087210939 2316.0312650203705\n",
      "66800 0.9029973492662681 2323.110409975052\n",
      "67000 0.9018968002592465 2330.1734187602997\n",
      "67200 0.9028118558308298 2337.315489053726\n",
      "67400 0.90324416629796 2344.388057231903\n",
      "67600 0.9028177693416224 2351.4286267757416\n",
      "67800 0.9029280038589137 2358.5946202278137\n",
      "68000 0.9024468723784493 2365.6211383342743\n",
      "68200 0.9028134977666321 2372.6923916339874\n",
      "68400 0.9028497308556953 2379.732433080673\n",
      "68600 0.9033687867983664 2386.820410013199\n",
      "68800 0.9027735567261461 2393.8991644382477\n",
      "69000 0.9031910152212086 2400.9733321666718\n",
      "69200 0.9028376031250376 2408.068414211273\n",
      "69400 0.9027081844690337 2415.1548340320587\n",
      "69600 0.9028707743137252 2422.259062767029\n",
      "69800 0.9029681392725504 2429.4281668663025\n",
      "70000 0.9030305298678483 2436.516410112381\n",
      "70200 0.9031644342154188 2443.621129989624\n",
      "70400 0.9028577446481157 2450.657289981842\n",
      "70600 0.9029430627765622 2457.7972638607025\n",
      "70800 0.903034384778525 2464.945201396942\n",
      "71000 0.9023000283653848 2472.084333896637\n",
      "71200 0.9033663967931307 2479.183506011963\n",
      "71400 0.9030754822048539 2486.171438932419\n",
      "71600 0.9033125297476834 2493.3123848438263\n",
      "71800 0.9030598426783594 2500.394622564316\n",
      "72000 0.9026158070020007 2507.476298570633\n",
      "72200 0.902942646633412 2514.5963089466095\n",
      "72400 0.9028950597207874 2521.707541704178\n",
      "72600 0.9031074049440407 2528.7648437023163\n",
      "72800 0.9029929014218809 2535.838794708252\n",
      "73000 0.9028843505823634 2542.915841817856\n",
      "73200 0.9031889821610366 2550.047731399536\n",
      "73400 0.9029398310089733 2557.1763496398926\n",
      "73600 0.9029800530942097 2564.308526992798\n",
      "73800 0.9031909636017278 2571.4115805625916\n",
      "74000 0.9029230612900597 2578.522678375244\n",
      "74200 0.9028481498155824 2585.655358314514\n",
      "74400 0.9026010845108838 2592.7070395946503\n",
      "74600 0.9029408444105131 2599.8325474262238\n",
      "74800 0.9029943161464291 2606.937331676483\n",
      "75000 0.9029177206813161 2614.072291135788\n",
      "75200 0.9032561108315318 2621.2375314235687\n",
      "75400 0.9027260947068464 2628.3559205532074\n",
      "75600 0.902869319847452 2635.5678055286407\n",
      "75800 0.902784766676166 2642.6308493614197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76000 0.9029374258090797 2649.801784515381\n",
      "76200 0.90296061090364 2656.949552297592\n",
      "76400 0.9028700181480838 2664.007131099701\n",
      "76600 0.9029766061386557 2671.0926179885864\n",
      "76800 0.9030393261445184 2678.163251876831\n",
      "77000 0.9030215634965981 2685.340672492981\n",
      "77200 0.9028818884119346 2692.511812210083\n",
      "77400 0.902992089382948 2699.556747674942\n",
      "77600 0.9032516631547804 2706.6229662895203\n",
      "77800 0.9028301854082851 2713.68381524086\n",
      "78000 0.9024795004041073 2720.8793802261353\n",
      "78200 0.9030435258001808 2728.0567808151245\n",
      "78400 0.9030141646333535 2735.1342861652374\n",
      "78600 0.9032371612346242 2742.22372341156\n",
      "78800 0.9031133745633245 2749.3046848773956\n",
      "79000 0.903119519443785 2756.463725566864\n",
      "79200 0.9029447655309417 2763.5426766872406\n",
      "79400 0.9029304212757526 2770.714805841446\n",
      "79600 0.902947138381122 2777.7948467731476\n",
      "79800 0.9030117616188263 2784.8870952129364\n",
      "80000 0.9032555483833347 2791.941512107849\n",
      "80200 0.9029572567356838 2799.06422495842\n",
      "80400 0.9030259852179604 2806.2158629894257\n",
      "80600 0.9030414536169886 2813.4040253162384\n",
      "80800 0.9030252499688758 2820.51300239563\n",
      "81000 0.9030437688241792 2827.617151260376\n",
      "81200 0.9030290575641052 2834.597752571106\n",
      "81400 0.9030814363181144 2841.624873161316\n",
      "81600 0.903168927622819 2848.7561378479004\n",
      "81800 0.9033260194044391 2855.8346931934357\n",
      "82000 0.9027768845372426 2862.9336302280426\n",
      "82200 0.9026114129273389 2870.040041446686\n",
      "82400 0.9030734532394995 2877.0506689548492\n",
      "82600 0.9031547557111554 2884.184394836426\n",
      "82800 0.9031088852893518 2891.342870950699\n",
      "83000 0.9030708701440393 2898.4155576229095\n",
      "83200 0.9022411471121243 2905.529565811157\n",
      "83400 0.9031191156438506 2912.5832414627075\n",
      "83600 0.9031255026427609 2919.7363786697388\n",
      "83800 0.9030753745263853 2926.707268714905\n",
      "84000 0.9018204110920429 2933.8125190734863\n",
      "84200 0.9030734219600506 2940.9424653053284\n",
      "84400 0.9029860097756327 2948.0612659454346\n",
      "84600 0.9031126105934872 2955.1249690055847\n",
      "84800 0.9029422264204359 2962.252073287964\n",
      "85000 0.9030572932129656 2969.410040140152\n",
      "85200 0.9030551158465712 2976.5303003787994\n",
      "85400 0.9031410733781027 2983.593177318573\n",
      "85600 0.9023562072525121 2990.6747510433197\n",
      "85800 0.9030580393203884 2997.7988011837006\n",
      "86000 0.9027340239010249 3004.872212409973\n",
      "86200 0.9034360610727529 3012.010906457901\n",
      "86400 0.9033936638710183 3019.059348344803\n",
      "86600 0.9030410223271066 3026.178811073303\n",
      "86800 0.9031820131672282 3033.292664051056\n",
      "87000 0.9031412294106498 3040.3925812244415\n",
      "87200 0.9028868328116175 3047.561450242996\n",
      "87400 0.9036263859444211 3054.6625463962555\n",
      "87600 0.9029115081719801 3061.799161195755\n",
      "87800 0.9029995249601132 3068.891764640808\n",
      "88000 0.9030395269826079 3075.9217760562897\n",
      "88200 0.902999871521037 3083.0396916866302\n",
      "88400 0.9036901816304965 3090.1339609622955\n",
      "88600 0.9030904213888753 3097.2798092365265\n",
      "88800 0.9031381069334218 3104.4565000534058\n",
      "89000 0.9031498564617618 3111.5460822582245\n",
      "89200 0.9035965097445743 3118.6647946834564\n",
      "89400 0.9033545863810724 3125.7898042201996\n",
      "89600 0.9031206997310057 3132.917731523514\n",
      "89800 0.9023749022704668 3140.0404856204987\n",
      "90000 0.9030820122864728 3147.1564297676086\n",
      "90200 0.9033568831252984 3154.240909576416\n",
      "90400 0.9032847953221473 3161.4140164852142\n",
      "90600 0.9031469158258396 3168.5301506519318\n",
      "90800 0.9031328161117419 3175.699048280716\n",
      "91000 0.903038678446396 3182.8252658843994\n",
      "91200 0.903230136297033 3189.9191603660583\n",
      "91400 0.9033199100758412 3196.9770278930664\n",
      "91600 0.9032528677454082 3204.1978330612183\n",
      "91800 0.9027517203909178 3211.332877635956\n",
      "92000 0.9032643508928087 3218.432695865631\n",
      "92200 0.9033199078236733 3225.5453069210052\n",
      "92400 0.9032395428442873 3232.7179188728333\n",
      "92600 0.9034452588023614 3239.781576871872\n",
      "92800 0.9030709788184381 3246.9462139606476\n",
      "93000 0.9035211842327082 3254.0984423160553\n",
      "93200 0.903098293957235 3261.1853556632996\n",
      "93400 0.9033917710813368 3268.236762523651\n",
      "93600 0.9031953429971932 3275.4040513038635\n",
      "93800 0.9036441779579524 3282.5022802352905\n",
      "94000 0.9031272642930013 3289.589277744293\n",
      "94200 0.9031910551362607 3296.7384486198425\n",
      "94400 0.9029299734442977 3303.8499851226807\n",
      "94600 0.9031572879197438 3310.99910569191\n",
      "94800 0.9036403115500884 3318.0573060512543\n",
      "95000 0.9031554573734369 3325.0795164108276\n",
      "95200 0.9030792578700262 3332.201169729233\n",
      "95400 0.9033389379088503 3339.306079387665\n",
      "95600 0.9034374541172232 3346.3785762786865\n",
      "95800 0.9030241627550158 3353.4789781570435\n",
      "96000 0.9031102899323299 3360.565772294998\n",
      "96200 0.9031278686452044 3367.708235502243\n",
      "96400 0.9033071146109778 3374.77610206604\n",
      "96600 0.9033080621831686 3381.8365576267242\n",
      "96800 0.9029679307680001 3388.8976786136627\n",
      "97000 0.9032094040258819 3395.964949607849\n",
      "97200 0.9030989709483448 3403.05712389946\n",
      "97400 0.9038262910775161 3410.181007385254\n",
      "97600 0.9032200969182066 3417.3068199157715\n",
      "97800 0.903244974230586 3424.538440465927\n",
      "98000 0.9031581044393372 3431.5852999687195\n",
      "98200 0.9027950080867415 3438.654394388199\n",
      "98400 0.9033826921146327 3445.713972091675\n",
      "98600 0.9035866254204854 3452.816549062729\n",
      "98800 0.9032590707648811 3459.8972659111023\n",
      "99000 0.9034640366029905 3467.00159907341\n",
      "99200 0.9030993207309562 3474.136151790619\n",
      "99400 0.9033912018385593 3481.2597465515137\n",
      "99600 0.9029529520137018 3488.3425352573395\n",
      "99800 0.9026021372147894 3495.4665474891663\n"
     ]
    }
   ],
   "source": [
    "mll = VariationalELBO(likelihood, model, num_data=train_V_ref.shape[0])\n",
    "#mll = VariationalELBO(likelihood, model, num_data=N)\n",
    "#epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n",
    "\n",
    "count = 0\n",
    "s = time.time()\n",
    "while count < 100000:\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss, pred, train_F = mll(output, train_V_ref, train_S_E, train_S_I)\n",
    "    #epochs_iter.set_postfix(loss=loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if count%200 == 0:\n",
    "        model.eval()\n",
    "        likelihood.eval()\n",
    "        test_output = model(train_x)\n",
    "        test_loss, test_pred, test_F = mll(test_output, test_V_ref, test_S_E, test_S_I)\n",
    "        \n",
    "        \n",
    "        test_score = metrics.explained_variance_score(y_true=test_V_ref.cpu().detach().numpy(),\n",
    "                                                      y_pred=test_pred.cpu().detach().numpy(),\n",
    "                                                      multioutput='uniform_average')\n",
    "        print(count, test_score, time.time() - s)\n",
    "        #print(likelihood.decay[1].item(), likelihood.shift[1].item())\n",
    "        \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T18:07:53.852897Z",
     "start_time": "2020-12-09T18:07:53.842997Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T23:58:11.479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd3241507f0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "plt.plot(test_V_ref.cpu().detach().numpy()[1000:4000])\n",
    "plt.plot(test_pred.cpu().detach().numpy()[1000:4000]-650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-09T23:58:11.480Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(10,17))\n",
    "\n",
    "for row in range(5):\n",
    "    for col in range(2):\n",
    "        axs[row, col].plot(test_F[col*5+row].cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
